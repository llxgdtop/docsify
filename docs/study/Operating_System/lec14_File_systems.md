## 课前内容

**UNIX** **系统的⽂件类型包括以下⼏种，该信息保存在stat** **结构中的st_mode** **成员中，普通⽂件。⽬录⽂件。块特殊⽂件。字符特殊⽂件。FIFO**。套接字。符号链接。

### ⼀、files and directory

存储虚拟化形成了两个关键的抽象。

第⼀个是⽂件File，⽂件可以看成⼀个线性字节数组，每个字节都可以读出或写⼊。每个⽂件都有某种低级名称（low-level name），通常称为inode number，⽤户⼀般不知道它们。

第⼆个抽象是⽬录Directory，⽬录也有inode number。⽬录的内容更具体：例如它是⼀个包含（⽤户可读名字，低级名字）对的列表，如（"foo"，10），因此"foo"所在的⽬录就会包含这个条⽬。⽬录中的每个条⽬都指向⽂件或其它⽬录，通过将⽬录放⼊其它⽬录中，⽤户就可以构建⽬录层次结构或⽬录树，并在该⽬录树下存储所有⽂件和⽬录，如下图所示。

![img](assets\clip_image002-1729428755331-1.jpg)

以UNIX ⽂件系统为例，⽬录层次结构从根⽬录开始，⼀般⽤"/"表示，并使某种分隔符（在UNIX 中就是"/"）来命名后续⼦⽬录，直到命名所需的⽂件或⽬录。⼀个⽬录或⽂件可以⽤绝对路径或相对路径来表示。绝对路径从根⽬录开始，因此路径名从"/"开始，如/foo/bar.txt；⽽相对路径从当前⼯作⽬录开始，因此路径名从当前⼯作⽬录中的⽬录或⽂件开始，例如当前⼯作⽬录为/bar 时，相对路径可以是bar、foo、foo/bar.txt。⽬录和⽂件可以具有相同的名称，只要它们位于⽂件系统树的不同位置，例如/foo/bar.txt 和/bar/foo/bar.txt。

 

⽂件描述符是⼀个⾮负整数，对于内核⽽⾔，所有打开的⽂件都通过⽂件描述符引⽤。当打开或创建

⼀个⽂件时，内核就向进程返回⼀个⽂件描述符，然后后续的读和写就使⽤该⽂件描述符来标识该⽂件，将其作为参数传递给read 和write 等函数。

int open(const char *path, int oflag, ... /* mode_t mode */);

它返回最⼩的未⽤描述符，输⼊参数如下：path是要打开或创建⽂件的名字，通常为绝对路径，如果希望使⽤相对路径，可以使⽤openat 函数。oflag ⽤来说明open 函数的多个选项，可以⽤以下⼀个或多个常量（如O_RDONLY、O_WRONLY 等等）进⾏或运算构成oflag 参数。最后是可选的mode 参 数，仅当创建新⽂件时，即oflag 设置了O_CREAT 时才使⽤该参数，它指定了新⽂件的访问权限位。

 

调⽤close函数可以关闭⼀个打开⽂件，同时释放该进程加在该⽂件上的所有记录锁。实际上，当⼀个进程终⽌时，内核⾃动关闭它的所有打开⽂件，很多程序利⽤这⼀点⽽不显式地调⽤close关闭打开⽂件。

 

每个打开⽂件都有⼀个与其相关联的当前⽂件偏移量，通常是⼀个⾮负整数，⽤于度量从⽂件开始处计算的字节数。⼀般读、写操作都从当前⽂件偏移量处开始，并使偏移量增加所读写的字节数。打开

⼀个⽂件时，除⾮指定了O_APPEND 选项，否则偏移量设置为0。

 

off_t lseek(int fd, off_t offset, int whence);

调⽤lseek 可以显式地对⼀个打开⽂件设置偏移量，参数offset 的解释与whence 的值有关。

●   若whence 是SEEK_SET，偏移量设置为距⽂件开始处offset 个字节。

●   若whence 是SEEK_CUR，偏移量设置为当前偏移量值+offset，offset 可正可负。

●   若whence 是SEEK_END，偏移量设置为⽂件⻓度+offset，offset 可正可负。

如果lseek 成功执⾏，则返回新的⽂件偏移量。如果⽂件描述符fd 指向的是⼀个管道、FIFO或⽹络套

接字等不能设置偏移量的⽂件，那么lseek 返回-1。

 

有⼏个值得注意的点。lseek 只是将当前的⽂件偏移量记录在内核中，本身并不引起任何I/O 操作，该偏移量被⽤于下⼀个读或写操作；⽂件偏移量可以⼤于⽂件的当前⻓度，此时对该⽂件的下⼀次写将加⻓该⽂件，并在⽂件中构成⼀个空洞，即位于⽂件中但没有被写过的字节都被读为0。⽂件空洞并不要求在磁盘上占⽤存储区，具体处理⽅式与⽂件系统实现有关：例如当定位超出⽂件尾端之后写时，对于新写的数据需要分配磁盘块，但是位于原⽂件尾端和新开始写的位置之间的部分，则不需要分配磁盘块。

 

⽂件偏移量：是指当前读写操作在⽂件中的起始位置。这个位置是从⽂件开头到当前操作点的字节数。当你打开⼀个⽂件并开始读写时，⽂件偏移量通常从0 开始，即⽂件的开头。随着你读取或写⼊数据，⽂件偏移量会相应地向前移动。（**有点类似于光标**）

⽂件⻓度：是指⽂件中数据的总字节数。这个⻓度从⽂件开头到⽂件末尾的距离。关系

⽂件偏移量是动态的，根据读写操作的进⾏⽽改变，⽽⽂件⻓度是⽂件当前包含的数据总量。当你在

⽂件的末尾写⼊数据时，如果⽂件偏移量超出了原有的⽂件⻓度，⽂件⻓度将会增加，以包含新增的数据。

 

调⽤read 从打开⽂件中读数据。如果read 成功，返回的是读到的字节数；如果已到达⽂件的尾端，则返回0；如果出错，返回-1。读操作会从⽂件的当前偏移量开始，在成功返回之前，该偏移量将增加实际读到的字节数。ssize_t 类型带符号，⽽size_t 类型不带符号。

有多种情况可以使得实际读到的字节数少于要求读的字节数：

●   读普通⽂件时，在读到要求字节数之前就到达了⽂件尾端。

●   从终端设备读时，通常⼀次最多读⼀⾏。

●   从⽹络读时，⽹络中的缓冲机制可能造成返回值⼩于所要求读的字节数。

●   从管道或FIFO读时，若管道包含的字节少于所需的数量，那么将只返回实际可⽤字节数。

●   从某些⾯向记录的设备（如磁带）读时，⼀次最多返回⼀个记录。

●   已经读了部分数据，但是被⼀个信号造成中断。

调⽤write 向打开⽂件写数据。和read 类似，write 成功时返回写⼊的字节数；否则在出错时返回-1。

write 出错的常⻅原因是磁盘已经写满，或者超过了⼀个给定进程的⽂件⻓度限制。

 

对于普通⽂件，写操作从⽂件的当前偏移量处开始。如果在打开⽂件时指定了O_APPEND 选项，则在每次写操作之前，都将⽂件偏移量设置在⽂件的当前结尾处，并在⼀次成功写⼊之后，该⽂件偏移量增加实际写⼊的字节数。

在⽂件末尾追加时，使⽤O_APPEND 选项和使⽤lseek 定位到⽂件尾端是不同的，主要不同在于操作的原⼦性：O_APPEND 使得内核在每次写操作之前，都先更新⽂件的当前偏移量，这使得写操作和更新偏移量合并成⼀个原⼦操作，即使多个进程追加同⼀⽂件，也不会出现覆盖的情况；⽽使⽤lseek ⽅法的话，写操作和更新偏移量是两个不同的函数，不具有原⼦性。

 

由dup返回的新⽂件描述符⼀定是当前可⽤⽂件描述符的最⼩值；⽽dup2 则可以⽤fd2 指定新⽂件描述符的值，如果fd2 已经打开，那么会先将其关闭，如果fd=fd2，则直接返回fd2，⽽不关闭fd2。两个函数返回的新⽂件描述符，与原来的fd 共享同⼀个⽂件表项。

 

UNIX 系统实现在内核中设有缓冲区⾼速缓存或⻚⾼速缓存，⼤多数磁盘I/O 都通过缓冲区进⾏。当我们向⽂件写⼊数据时，内核通常先将数据复制到缓冲区中，然后排⼊队列，晚些时候再写⼊磁盘，这种⽅式称为延迟写。当内核需要重⽤缓冲区来存放其它磁盘块数据时，它就会把所有的延迟写数据块写⼊磁盘。

UNIX  系统提供了sync、fsync和fdatasync三个函数，来保证磁盘上实际⽂件系统与缓冲区中内容的

⼀致性。

●   sync只是将所有修改过的块缓冲区加⼊写队列中，然后就返回，并不等待实际的写磁盘操作结

束。通常，系统守护进程update会周期性地（例如30 秒）调⽤sync，从⽽保证定期冲洗flush

内核的块缓冲区。

●   fsync只对fd 指定的⼀个⽂件起作⽤，并且会阻塞地（或者说同步地）等待写磁盘操作结束以后才返回，该函数可被⽤于需要确保修改过的块⽴即写到磁盘上的应⽤程序，如数据库等。

●   fdatasync类似fsync，但只影响⽂件的数据部分，⽽fsync还更新⽂件的元数据（或属性）。调⽤stat 和fstat 可以查看特定⽂件的元数据。

 

元数据是关于数据的数据，它提供了数据的详细信息（⽐如权限，⽂件⼤⼩等等）但不是数据内容本身。在不同的上下⽂中，元数据有不同的形式和⽤途，但主要⽬的是描述数据的特征，使数据的检索和管理更加⾼效和有序

 

调⽤link 可以创建⼀个指向现有⽂件的硬链接，⽽调⽤unlink可以删除⼀个⽬录项，并且将pathname所引⽤的⽂件的硬链接计数减1，如果该⽂件还有其它硬链接，仍可通过其它链接访问该⽂件的数据。在第⼀章中我们介绍过硬链接，即每个inode节点都有⼀个链接计数，值是指向该inode节点的⽬录项数，只有当链接计数减⾄0 时，⽽且没有进程打开该⽂件，才可删除该⽂件并释放占⽤的数据块。因此，关闭⼀个⽂件时，内核⾸先检查打开该⽂件的进程数，如果是0，再去检查⽂件的硬链接个数，如果也是0，那么就删除该⽂件的内容。创建⽂件的硬链接之后，原有⽂件名和新创建的⽂件名之间没有区别，它们都指向同⼀个inode节点。

unlink的特性使得，即使程序崩溃，它所创建的临时⽂件也不会遗留下来。进程调⽤open 或creat 创建⼀个⽂件，然后⽴即调⽤unlink，因为该⽂件仍处于打开状态，所以内核不会⻢上删除其内容。只

 

有当进程关闭该⽂件（显式地调⽤close），或终⽌时（内核关闭该进程打开的全部⽂件），该⽂件的内容才会被删除。

通常来说，我们使⽤unlink删除⽂件，⽽使⽤rmdir删除空⽬录，后续还会强调这⼀点。或者，使⽤ remove 删除⽂件或⽬录的链接，对于⽂件它就像unlink，对于⽬录它就像rmdir。

 

符号链接，⼜称为软链接，这是单独⼀种⽂件类型。虽然从结果上看，我们也能⽤新的别名来访问⽂件，但是本质上完全不同。⼀是⽂件类型不同，⼆是符号链接⽂件的数据是链接指向⽂件的路径名，三是存在悬空引⽤问题，即删除原始⽂件后，会导致符号链接指向不再存在的路径名，因此⽆法访问原来的⽂件（可以将其理解成快捷⽅式）；⽽使⽤硬链接的话，即使删除多个别名，只要还存在硬链接，就可以通过那个链接访问原⽂件。

 

链接计数、硬链接和符号链接这⼏个概念的不同。

每个inode结点都有⼀个链接计数nlink，只有当链接计数减为0 时，才可删除该⽂件，即释放该⽂件占⽤的磁盘块（这⾥还没有说明inode底层的⽂件类型是什么）。

如果inode是普通⽂件类型，那么nlink既是链接计数，也是硬链接的计数（因为符号链接是另⼀种⽂件类型，不记在原inode的nlink下），

但如果inode是⽬录类型，那么nlink并不代表硬链接，因为我们说过不能创建⽬录的硬链接，因此该 nlink的含义只是链接计数。通常，任何⼀个叶⽬录（不包含任何其它⽬录的⽬录），其链接计数总是为2，来⾃于命名该叶⽬录的⽬录项，以及叶⽬录的"."项（上⼀个⽬录以及它⾃⼰）；⽽⾮叶⽬录的链接计数⾄少为3，⾄少有⼀个额外的链接计数，来⾃于其⼦⽬录的".."项。（..及上⼀层⽬录）



**在⽂件系统中，硬链接（Hard Link）和软链接（Symbolic Link 或 Symlink）是⽤来引⽤⽂件的两种不同⽅式，⽽链接计数（Link Count）则是与硬链接相关的⼀个概念。这三者之间有着明显的区别：硬链接（Hard Link）**

**硬链接是指向⽂件系统中某个⽂件的inode（索引节点）的⼀个直接指针。⼀个⽂件可以有多个硬链 接，每个链接都等同于原⽂件，拥有相同的inode号。修改任何⼀个硬链接的内容都会反映在所有链接上，因为它们实际上指向的是同⼀个⽂件数据。**

**链接计数：每个⽂件的inode结构中有⼀个链接计数器，该计数器记录了指向该inode的硬链接数。当创建⼀个硬链接时，链接计数会增加；当删除⼀个硬链接时，链接计数会减少。当链接计数为零时，系统会释放该⽂件占⽤的空间。**

**软链接（Symbolic Link）**

**软链接或符号链接，是⼀个特殊类型的⽂件，它包含的是另⼀个⽂件的路径名引⽤⽽不是直接指向 inode。软链接可以跨⽂件系统，可以链接到⽬录，也可以包含不存在⽂件的路径。修改软链接不影响原始⽂件，但如果原始⽂件被删除，软链接将变为⽆效。**

**例⼦**

**假设我们有⼀个⽂件original.txt，我们为它创建⼀个硬链接hardlink.txt 和⼀个软链接symlink.txt。当我们查看这两个链接⽂件时，hardlink.txt将与original.txt 表现得完全相同，因为它们共享同⼀个 inode。**

**symlink.txt将以路径的形式指向original.txt。如果original.txt 被删除，symlink.txt仍然存在但会指向**

**⼀个不存在的⽂件，因此会出现错误，⽽hardlink.txt仍然可以访问原始数据，因为它与原始⽂件共享 inode。**

 

在管理⽂件时，选择硬链接或软链接取决于具体的需要，如是否需要跨⽂件系统链接，或是否希望链接指向的⽂件被删除后还能保持有效。

 

调⽤mkdir 可以创建⼀个空⽬录，其中"."和".."条⽬会⾃动创建。传⼊⽂件访问权限mode 时，应该注意，对于⽬录通常⾄少要设置⼀个执⾏权限位，以允许访问该⽬录中的⽂件名。

调⽤rmdir可以删除⼀个空⽬录，空⽬录只包含"."和".."条⽬。如果调⽤rmdir 使得该⽬录的链接计数变为0，且没有其它进程打开此⽬录，则释放由此⽬录占⽤的空间。

调⽤chdir可以更改当前⼯作⽬录，当前⼯作⽬录是搜索所有相对路径名的起点。当前⼯作⽬录是进程的⼀个属性，因此它只影响调⽤chdir的进程本身，并不影响其它进程。shell 也是同样的道理，shell的当前⼯作⽬录不会随着程序调⽤chdir⽽改变，因此，为了改变shell 进程⾃⼰的⼯作⽬录，shell 应该直接调⽤chdir，这就是为什么cd命令内建在shell 中。

 

电脑上有⼀个名为“Documents”的⽬录。在这个⽬录中，你可能有多个⽂件如“resume.docx”和  “budget.xlsx”，以及⼀个⼦⽬录“Photos”。在⽂件系统中，“Documents”是⼀个⽬录，⽽  “resume.docx”、“budget.xlsx”和“Photos”是这个⽬录下的⽬录项。





### ⼆、file system implement

我们设计的极简⽂件系统的磁盘结构如下图所示。

![img](assets\clip_image002-1729428905779-3.jpg)

⾸先，我们将磁盘分成块，简单⽂件系统只⽤⼀种块⼤⼩，我们选择常⽤的4KB。因此，对于⼀个磁盘分区，我们只需将其看成⼀系列的块，这⾥我们假设有⼀个很⼩的磁盘，只有64 块。

然后，我们就要开始考虑，在这些块上存储什么。

第⼀个想到的是⽤户的数据，实际上，任何⽂件系统中⼤多数空间应该⽤于存放⽤户数据。这些⽤于存放⽤户数据的磁盘区域，称为数据区域，这些块因此称为数据块data block。简单起⻅，我们将后 56 个磁盘块都作为数据块。

⽂件系统必须记录关于每个⽂件的⼤量信息，我们将这些数据组织成元数据metadata，例如前⾯我们看到的struct stat，元数据包括⽂件包含哪些数据块、⽂件⼤⼩、⽂件所有者和访问权限、访问和修改时间等类似信息。为了存储这些信息，⽂件系统通常使⽤⼀种叫做inode的结构，存放inode所使⽤的磁盘区域，我们称为inode表inode table 或inode block。它可以看成是⼀个保存磁盘上inode 的数组，在这⾥，我们使⽤了5 个块作为inode block。

 

⾄此，我们有了data block和inode block，但是还需要某种⽅法来记录这些块是空闲的还是已分配 的，因此，我们选择⼀种简单结构⸺位图bitmap。我们使⽤磁盘上的2 块，⼀块作为data bitmap，另⼀块作为inode bitmap。位图的使⽤很简单，它⽤每个位指示相应的块是空闲(0) 还是正在使⽤(1)。最后，我们留出⼀块，也是磁盘上的第⼀块，将其作为超级块superblock。超级块包含关于该⽂件系统的特定信息，如⽂件系统有多少个inode block 和data block、inode table 的开始位置、⼀些标识⽂件系统类型的幻数等等。

 

在挂载⽂件系统的时候，操作系统⾸先读取超级块，初始化各种参数，然后将该卷添加到⽂件系统树上，当卷中的⽂件被访问时，系统就会知道在哪⾥查找所需的磁盘上的结构。（在计算机术语中，挂载文件系统指的是将一个文件系统连接到主文件系统的树中的一个过程。这使得操作系统可以访问并使用该文件系统中的文件和目录。挂载是一个关键步骤，使得存储设备（如硬盘驱动器、USB驱动器或网络存储）上的数据能够在计算机系统中被访问和管理。当你将一个存储设备连接到计算机时，例如通过USB插入一个外部硬盘或闪存驱动器，操作系统需要某种方式来读取和理解这个设备上的数据。挂载过程就是建立这种连接的步骤，它通常包括以下几个方面：

识别文件系统类型：操作系统识别出设备使用的文件系统类型（如FAT32、NTFS、ext4等）。

分配挂载点：挂载点是文件系统树中的一个位置，所有通过该挂载点访问的请求都会被重定向到挂载的文件系统。例如，在Unix和Linux系统中，你可能将一个USB驱动器挂载到/mnt/usb目录。

读取文件系统：操作系统读取文件系统的元数据，如目录结构、文件属性等，以便可以通过操作系统的文件管理系统访问这些文件。）

 

下图是典型的UNIX ⽂件系统设计，你可以将它和我们上⾯设计的极简⽂件系统相对⽐。

![img](assets\clip_image004-1729428905779-4.jpg)

典型UNIX ⽂件系统的布局设计

下⾯我们讨论这个简单⽂件系统的更多细节和其它⽅⾯。

inode

inode是index node 的缩写，在许多⽂件系统中都被使⽤，⽤于保存和描述给定⽂件的元数据的结构。inode号可以⽤于索引磁盘上的inode数组，即每个inode都由⼀个数字（inumber，即inode number）隐式引⽤，只要给定⼀个inumber，就可以通过计算得出磁盘上相应inode节点的位置。例如，在我们的简单⽂件系统中，inode block 有5 个，每个4KB，假设inode⼤⼩是256B（xv6是64B），那么每个inode block 就可以保存16 个inode，因⽽该⽂件系统有80 个inode。⽽再根据inode table 从第4块，即从12KB地址开始，我们就可以有以下的视图。

![img](assets\clip_image006-1729428905779-5.jpg)

inode block 的详细视图

 

因此，特定inode的地址可以根据inode table 的起始地址，加上inode区域的偏移量来得到。例如要读取inode32，则该inode的地址是12KB+32×256B=20KB。

但是，磁盘并不是按字节寻址的，⽽是由⼤量的可寻址扇区组成，扇区⼤⼩通常为512B。因此为了读取inode32，⾸先我们需要找出该扇区的地址，20KB÷512B=40，所以⽂件系统将向扇区40 发出⼀个读取请求，然后就会获得inode32 和inode33，从⽽获得我们期望的块。

更⼀般地，inode块的扇区地址可以计算如下。（第一个记得要加上初始的块，如上则为第4块）

![img](assets\clip_image008-1729428905779-6.jpg)

sector即扇区，一个扇区通常存储固定数量的数据；最常见的大小是512字节或更现代的4096字节（通常称为4K扇区，像上面那个简单介绍一样）。**扇区是硬盘读写操作的最小单位**，即硬盘在进行读写数据时，最少会读写一个扇区的数据。（现代一个扇区4kb，多数现代文件系统中的块block大小也为4kb，即一块就是一个扇区，但像ssd可能是8kb或者16kb等，像xv6一个扇区是512B，一个block为1kb，即一个块有两个扇区)。

inode（索引节点）可以被视为一个结构体，它在文件系统中用来存储有关文件的元数据。在编程和操作系统的术语中，结构体是一种数据结构，它可以封装多个不同类型的数据项。同样地，一个inode结构体包含了多种类型的信息，例如文件的权限、大小、拥有者、时间戳、数据块位置等。

 

 

多级索引

inode包含了有关该⽂件的数据块驻留在磁盘上位置的信息，设计inode时，最重要的决定之⼀就是如何引⽤数据块的位置。⼀种简单⽅法是，在inode中有⼀个或者多个直接指针，指向磁盘某个地址，因此，每个指针都可以指向属于该⽂件的⼀个磁盘数据块。

但是，如果你的⽂件⾮常⼤，⼤于块的⼤⼩× 直接指针数，那么就需要不同的结构。⼀种常⻅的思想就是使⽤间接指针，如下图所示。（filemetadata 就是type nlink size这些）

 ![img](assets\clip_image010-1729428905779-7.jpg)

间接指针不指向包含⽤户数据的块，⽽是指向包含更多指针的块。因此，在设计inode时，可以有⼀些固定数量的直接指针，和⼀个或多个间接指针。间接指针的数量和等级，可以基于所需⽀持的最⼤⽂件⼤⼩来设定。例如，我们假设指针的总数是13，有10 个直接指针，1 个间接指针，1 个双重间接指针和1 个三重间接指针，那么它⽀持的⽂件最⼤⼤⼩为：

![img](assets\clip_image012-1729428905779-8.jpg)

许多⽂件系统都使⽤多级索引，如Linux的ext2、ext3、原始UNIX ⽂件系统等。你可能想知道为什么要使⽤这样的不平衡树的结构，⽽不是其它数据结构？许多研究⼈员研究过⽂件系统以及它们的使⽤⽅式发现⼤多数⽂件都很⼩，因此，我们采⽤不平衡的设计，即只需要⽤少量的直接指针，就可以解决⼤部分的⽂件存储问题，实现性能上的优化；需要处理⼤⽂件时，再使⽤⼀个或多个间接块来处理。

 

⽬录组织

⽬录的组织⼀般很简单，⼀个⽬录基本上就是⼀个列表，列表中包含⼀个核⼼的⼆元组（条⽬名称，

inode number）。⼀个例⼦如下图所示。

![img](assets\clip_image014.jpg)

⼀个简单的⽬录内容

以上图为例，每个条⽬都有⼀个inode number，⼀个记录⻓度（名称字节数+ 剩余空间），⼀个字符串⻓度（名称实际⻓度），以及条⽬的名称。

通常，⽂件系统将⽬录视为特殊类型的⽂件，因此，⽬录也有⼀个inode，该⽬录具有由inode指向的

⽬录的数据块，在这些数据块中就保存着上图中的数据。

使⽤简单的线性⽬录列表不是唯⼀⽅式，例如XFS 使⽤B树形式存储⽬录。

 

空闲空间管理

⽂件系统需要记录哪些inode块和数据块是空闲的，位图是其中⼀种⽅法，早期的⽂件系统使⽤空闲列表⽅式，⽽现代⽂件系统使⽤更复杂的数据结构，例如B树等。

为新⽂件分配数据块时，可能会考虑⼀些其它事项。例如Linux的ext2、ext3 在创建新⽂件并需要分配数据块时，⾸先寻找⼀系列空闲块，将它们分配给新创建的⽂件。⽂件系统希望保证⽂件的⼀部分在磁盘上是连续的，这样可以提⾼性能，因此，采⽤预分配策略是不错的选择。

 

 

访问路径：从磁盘读取⽂件

现在我们关注读取和写⼊⽂件系统的过程，我们假设⽂件系统已被挂载，超级块在内存中，⽽所有其它内容如inode、⽬录等都还在磁盘上。

⾸先我们关注从磁盘读取⽂件的访问路径，我们希望打开⼀个⽂件/foo/bar，读取它然后就关闭它，我们假设⽂件⼤⼩只有4KB即刚好⼀块，下⾯是这个操作的访问路径。

●   调⽤open("/foo/bar", O_RDONLY)。为了找到⽂件，我们要遍历完整路径名。

●   遍历从⽂件系统的根⽬录开始，因此第⼀次磁盘读取是根⽬录的inode，⽽根⽬录的inode号是

 众所周知的，在UNIX ⽂件系统中是2。

●   接着，有了根⽬录的inode，就可以读取根⽬录的数据块，从⽽读取根⽬录的内容，以寻找foo条⽬，然后我们就要读取foo 的inode。依此类推，我们继续读取foo 的数据块和bar的inode。这时，open 就将分配⼀个⽂件描述符给⽂件bar，然后返回给⽤户。

●   ⽂件bar已被打开，利⽤open 返回的⽂件描述符，我们调⽤read 来读取⽂件的数据。

●   第⼀次read 的读取会从偏移量0 开始，因此将读⽂件的第⼀个块，查阅bar的inode找到这个块的位置，然后读取bar的数据块data[0]，并且需要更新⽂件的最后访问时间，因此也要更新 bar的inode。以此类推，下⼀次就是bar的数据块data[1]、data[2]……，⽽且每次读完之后都要更新⽂件的最后访问时间，因此每次都要重新写⼊bar的inode。

从磁盘上读取⽂件的整个访问路径可以⽤下图展示。

![img](assets\clip_image016.jpg)

从磁盘读取⽂件的时间线（向下时间递增）

 

访问路径：写⼊磁盘

下⾯我们关注向磁盘写⼊⽂件的访问路径，同样是⽂件/foo/bar，⽂件应该先被打开，然后程序调⽤ write 更新⽂件，最后关闭⽂件，看起来和上⾯读取的过程类似。

但是，如果写⼊⽂件需要分配新的块时，例如写⼊⼀个新创建的⽂件时，事情就变得麻烦了。因为写

⼊⼀个新⽂件时，每次写⼊操作不仅需要将数据写⼊磁盘，还必须先决定将哪个块分配给⽂件，从⽽需要更新磁盘的相应结构，如位图和inode。

因此，考虑写⼊新⽂件的话，逻辑上会产⽣5 次I/O：

●   读取⽂件的inode。

●   读取数据位图，在内存中更新该位图以标记新分配的块被使⽤。

●   写⼊新分配的数据块。

●   写⼊数据位图，将其新状态存⼊磁盘中。

●   写⼊⽂件的inode，更新新数据块的位置，⽂件的最后访问时间。

 

如果考虑创建新⽂件的话（在⼀个已有的⽬录中创建新⽂件），同样也需要更多的I/O：

 

●   读取根⽬录的inode和根⽬录的数据块。

●   读取⽂件所在⽬录（如foo）的inode 和数据块。

●   读取inode位图，在内存中更新它，因为准备创建⼀个新⽂件，所以要创建⼀个新inode。

●   写⼊⽂件所在⽬录（如foo）的数据块，创建新的⽬录条⽬。

●   写⼊inode位图，将其新状态存⼊磁盘中。

●   读取新⽂件的inode。

●   写⼊新⽂件的inode，更新其修改/ 访问时间。

●   写⼊⽂件所在⽬录（如foo）的inode，因为添加了新条⽬，更新其修改时间。

创建新⽂件并且写⼊的整个访问路径可以⽤下图展示。

![img](assets\clip_image018.jpg)

缓存和缓冲

通过前⾯从磁盘读取和写⼊磁盘的例⼦，你可能发现每次对⽂件的读取和写⼊，代价可能会是昂贵 的，会导致很多磁盘I/O，如果磁盘是慢速的，这就是⼀个巨⼤的性能问题。为了弥补这种缺陷，⼤多数⽂件系统积极使⽤系统的物理内存DRAM 来缓存重要的块。

如果没有缓存，每个打开的⽂件都需要对⽬录层次结构中的每个级别进⾏⾄少两次读取（⼀次是相关

⽬录的inode，⾄少⼀次是⽬录的数据块），特别是，路径名很⻓的情况下，仅仅是为了打开⽂件，可能就需要数百次I/O。

现代系统通常采用动态划分缓存的方法，比如LRU，比如写缓冲（即延迟写入）。





### 三、Fast File System

快速文件系统Fast File System。FFS的思想是让文件系统的结构和分配具有"磁盘意识"，从而提高性能。FFS以及后来的现代文件系统都遵循现有的接口以保持兼容性，而改变其内部的实现。

 

Basic Structure: The Cylinder Group

首先要更改磁盘上的结构。FFS将磁盘划分为一些分组，称为柱面组Cylinder Group。一个具有10个柱面组的磁盘如下图所示。

![img](assets\clip_image002-1729428964941-18.jpg)一个磁盘，由10个柱面组组成

 

使用柱面组是FFS用于改善性能的核心机制，通过在同一组中放置两个或多个文件，FFS就可以保证先后访问这两个或多个文件时，不会导致穿越磁盘的长时间寻道。柱面组在磁盘驱动器上的位置如下图所示，在同一个柱面组内不需要额外的寻道操作。FFS需要在每个柱面组中分配文件和目录，一个柱面组的布局如下图所示。

![img](assets\clip_image004-1729428964941-19.jpg)观察它的构成，为了可靠性，每个组都有超级块S的一个副本，因此如果一个柱面组的超级块被损坏或划伤，仍可以使用别的副本来访问文件系统。在每个柱面组中，使用inode位图和数据块位图，来记录该柱面组的inode和数据块是否已分配。

 

Allocating Files and Directories

现在我们定义了柱面组的结构，FFS的原则很简单，将相关的文件和目录以及相关元数据放置在同一个柱面组中，而不相关的东西则放在不同的柱面组中。

为实现上述目标，FFS使用一些简单的放置推断方法。

首先是目录，FFS找到已分配数量少的柱面组（我们希望跨组平衡目录），和大量的空闲inode（随后可能分配很多的文件），并将目录数据和inode放在该分组中。

然后对于文件，FFS先确保，在一般情况下，将文件的数据块分配到与其inode相同的柱面组中，防止从inode到数据块之间发生长时间寻道。其次，FFS将位于同一目录中的所有文件，放在它们所处目录的柱面组中。

 

The Large-File Exception

FFS的文件放置策略有一个重要例外情况，就是大文件的放置。对于大文件，FFS先将一定数量的块分配到第一个柱面组中（例如可以是inode中可用的直接指针的数量），然后将文件的下一个大块（例如可以是第一个间接块指向的部分）放入另一个块组中，依此类推。下面是使用大文件例外策略的前后对比图，这里大文件大小为10个块。

![img](assets\clip_image006-1729428964941-20.jpg)将大文件以大块的形式分布在磁盘上，而在磁盘上分散文件块会损害性能，特别是顺序访问该文件时。我们可以通过选择大块的大小来改善这一点。如果大块大小足够大，那么大部分时间仍然被花在从磁盘传输数据上，而在大块之间寻道的时间相对较少。每次开销做更多工作，从而减少开销，这种思想称为摊销amortization。

 

### 四、Crash-Consistency Problem

文件系统面临的一个主要挑战在于，在出现断电和系统崩溃的情况下，更新文件系统的数据结构。如何确保文件系统将磁盘上的映像保持在合理的状态？这个问题，称为崩溃一致性问题Crash-Consistency Problem。

更新实例

我们先考虑一种简单的工作负载，它将单个数据块附加到原有文件。因此，在文件关闭前，会向文件发出单个4KB写入来完成内容的追加。

假定磁盘使用上一章中讨论的极简文件系统，那么文件更新前的状态可能如下图所示。

向文件追加一个新数据块，需要更新文件的inode，新的数据块Db以及数据块位图三个块，这3个块必须被写入磁盘，因此文件更新完成后的状态可能如下图所示。

为了完成这次更新，文件系统需要对磁盘执行3次单独写入。但是，这些写操作通常不会立即发生，很大概率会被延迟写入，在缓冲区缓存中存在一段时间，然后文件系统过5s—30s之后再决定将它们写入磁盘。问题是，如果在这些写入的一个或两个完成以后发生断电或崩溃，而不是全部3个都完成，那么文件系统可能会处于奇怪的状态。

![img](assets\clip_image008-1729428964941-23.jpg)崩溃场景

假设只有一次写入成功。

只将数据块Db写入磁盘。新数据块Db确实被写入了磁盘上，但是没有指向它的inode，也没有表示该块已分配的位图，因此，这个写入就像没有发生过一样。

只将I[v2]即更新后的inode块写入磁盘。这时，inode指向了新数据块Db所处的磁盘地址，但是Db没有被写入到磁盘上，因此，我们可能会读出该数据块的旧内容，即垃圾数据。

只将更新后的位图B[v2]写入磁盘。这时，位图指示新的数据块Db已经分配，但是没有指向它的inode，因此，可能会导致空间泄露，因为文件系统将永远不会使用这一块。

注意到，上面的第2和第3种情况都出现了文件系统的数据结构不一致，因为inode和位图提供的信息相互矛盾。

还有情况是，两次写入成功，最后一次失败。

I[v2]和B[v2]成功写入。虽然文件系统元数据一致，但问题仍是Db的位置可能是垃圾数据。

I[v2]和Db成功写入。inode指向了正确的数据块Db，但是位图和inode又发生冲突，因此数据可能会被覆盖，因为位图指示Db未被分配。

B[v2]和Db成功写入。位图现在指示Db已被分配，inode和位图再次发生冲突，虽然位图正确指示了Db被分配，但是我们不知道其属于哪个文件，因为没有inode指向它。

 

Solution 2: Data Journaling（1是file system check fsck，不好用）

日志记录Journaling，或称为预写日志Write-Ahead Logging。和fsck的思路不同，fsck总是尝试在事情发生以后，一次性地全部补救，而日志记录虽然为每次的写入都增加一点开销，但可以更快地从断电或崩溃中恢复。现代文件系统，如Linux的ext3、ext4，Windows的NTFS，都使用这种想法。基本的思路是，每次在更新磁盘，即将覆写相应数据结构时，先写下一些记录/注释在磁盘的其它位置上，这个记录/注释描述了我们将要做的事情。在写下记录/注释的这个磁盘位置，我们就将这些记录/注释组织成日志。

通过每次将少量记录/注释写入磁盘，在更新磁盘上的数据结构发生崩溃时，可以倒回查看我们的记录/注释，然后重试这些操作。因此，我们可以在崩溃后，通过日志准确地知道需要修复的内容，而避免了调用fsck扫描整个磁盘，大大减少了恢复期间所需的工作量。使用日志的开销，是在我们更新期间的增加的一点额外工作量。

![img](assets\clip_image010-1729428964941-21.jpg)

以前面讨论的更新实例为例，我们需要更新的仍是三个块：inode块I[v2]、位图B[v2]和新数据块Db。在将它们写入磁盘的确切位置之前，我们先将其写入日志，如下图所示。

![img](assets\clip_image012-1729428964941-22.jpg)

日志的组成（一个事务）

可以看到，除了我们需要写入的三个块，还有事务开始标记TxB和事务结束标记TxE。TxB告诉我们有关这次更新的信息，例如三个块最终要写入的地址，并包含事务标识符TID；中间三块是确切的内容，我们确切地将这三块写到了磁盘的日志上；TxE是这次事务结束的标记，同样包含TID。

 

一旦以上完整的结构，即TxB——写入块——TxE安全地存在于磁盘的日志上，这时就可以开始覆写磁盘上文件系统的旧数据结构了，这个过程称为加检查点Checkpointing。现在我们开始对文件系统加检查点，将I[v2]、B[v2]和Db写入磁盘的相应位置上，如果这些写入全都顺利完成，那么这个加检查点的过程就成功。

 

这看起来是一个很简单的两步过程：日志写入+加检查点。但是现实情况是，因为延迟写入，我们往往会将5个块的写入转换为单个顺序写入。但是，给定如此大的写入，磁盘内部可以执行调度，并以任何顺序完成大批写入的块。这意味着，我们有可能先成功写入TxB和TxE，再去写入剩下3个实际数据块。如果不幸在这两个过程之间发生断电或崩溃，那么我们的日志就出现了问题，因为3个数据块中的一个或几个并没有被写入到日志上。

 

看起来完整的日志中缺失了一块或几块，对于其它的情况可能会是致命的，例如，当缺失的块是超级块时，可能会导致文件系统无法挂载。

 

因此，日志写入又可以再分为两步，如下图所示。首先将TxE以外的块都写入日志，这些写入可以同时发出，而不管其写入顺序如何；在这些块都写入完成后，文件系统才会发出TxE的写入，从而使日志处于一个完整正确的状态。因此，现在看起来，数据日志的三部曲如下：

日志写入：将TxB，元数据块和数据块写入日志，等待这些写入完成。

日志提交：将TxE写入日志，等待写入完成。一旦完成，我们认为事务已经成功提交。

加检查点：将日志中的事务（元数据和数据更新）写入磁盘的最终位置上。

 

现在我们来考虑文件系统如何利用日志，从断电或崩溃中恢复。如果崩溃或断电发生在上述第2步完成以前，即事务被提交以前，那么文件系统将简单地跳过这次更新，就像什么事情也没发生过一样；如果在第2步完成后，执行第3步加检查点的过程中发生崩溃，那么文件系统可以在系统引导阶段，扫描日志并查找到已成功提交的事务，然后重放这些事务，再次尝试将事务中的元数据和数据写入到磁盘的最终位置上。

 

此外，为了降低大量额外的磁盘流量，一些文件系统不会一次一个地向磁盘提交每个更新，而是将所有更新缓冲到全局事务中，最后提交包含所有更新的单个全局事务。因此，和延迟写入类似，通过缓冲更新，文件系统在很多情况下避免了对磁盘过多的写入流量。

 

最后，我们应该知道，日志位于磁盘上，因此它的大小是有限的，如果不断往日志中添加事务，那么它很快就会被填满。这时可能出现两个问题：随着日志变得越来越满，恢复时间也会越来越长，因为日志中需要按顺序重放的事务也会越来越多；当日志变满或接近满时，不能再向磁盘进一步提交事务。

 

为此，日志文件系统将日志视为循环数据结构，如下图所示。为了在日志中腾出空间以便后续事务提交，一旦某个事务的加检查点过程成功，那么就应该释放其在日志中占用的空间，以允许重用该空间。一个简单的方法是，在日志超级块中标记日志中最旧和最新的事务，因此其余的空间都是空闲的，如下图所示。

![img](assets\clip_image014-1729428964941-24.jpg)因此，数据日志最终的四部曲应当如下：

日志写入：将事务的内容（TxB，元数据块和数据块）写入日志，等待这些写入完成。

日志提交：将事务提交块（包括TxE）写入日志，一旦完成，我们认为事务已经成功提交。

加检查点：将日志中的事务（元数据和数据更新）写入磁盘的最终位置上。

释放：一段时间后，更新日志超级块，将已完成加检查点的事务标记为空闲。

现在，我们得到了最终的数据日志协议。但是一个问题是，我们将每个更新数据块（元数据和数据）写入磁盘两次，这可能会是一个昂贵的成本。

 

Solution 3: MetaData Journaling

与数据日志相对，这种日志形式称为元数据日志Metadata Journaling。

元数据日志与数据日志的协议几乎相同，只是不将用户数据写入日志中，继续以前面的更新例子为例，那么用户数据Db没有被写入元数据日志中，而是直接写入磁盘的最终位置上。新的问题是，我们应该何时将用户数据写入磁盘上？

 

用户数据应该在相关元数据写入磁盘之前完成，或者，应该在事务提交之前完成。值得注意的是，目录的内容也被看作是元数据。

 

元数据日志的协议如下：

数据写入：将用户数据写入到磁盘最终位置上，等待其完成。

日志元数据写入：将事务的内容（TxB和元数据）写入日志，等待写入完成。

日志提交：将事务提交块（包括TxE）写入日志，一旦完成，我们认为事务已经成功提交。

对元数据加检查点：将日志中的事务（元数据更新）写入磁盘的最终位置上。

释放：一段时间后，更新日志超级块，将已完成加检查点的事务标记为空闲。

 

先写入被指对象，再写入指针对象，这一规则是崩溃一致性的核心。通过强制先写入用户数据，文件系统可以保证指针永远不会指向垃圾数据。不过唯一的真正要求是，在事务提交发出之前（即TxE写入日志之前），完成数据写入和日志元数据写入，即在上述协议的第3步开始之前，第1和第2步必须完成，而第1和第2步的具体顺序不作限定。



### 五、xv6 beside logging

![img](assets\clip_image002-1729429014544-32.jpg)

xv6的文件系统总共分为7层：

最底层的磁盘层Disk Layer，与QEMU仿真的虚拟磁盘打交道，往磁盘上读或写一些块。

缓冲区缓存层Buffer Cache Layer，负责将磁盘块缓存在内存中，并且在这一层管理所有进程对缓存块的并发访问，保证一次只能有一个进程修改某一缓存块。

日志层Logging Layer，为高层提供更新磁盘的接口，高层的对几个磁盘块的更新，将被打包成事务放入日志层，日志层随后确保这些更新是原子的，并且能提供Crash Recovery。

inode层Inode Layer，为文件层提供接口，每个文件都是独立的，且有一个唯一的inode号标识，inode里面还有指向文件数据块所在磁盘位置的信息。

目录层Directory Layer，目录被看成是一种特殊的文件，因此它的inode含义也和普通文件不同，它的数据是一系列的目录条目，包含该目录下的文件名和文件的inode号。

路径名层Pathname Layer，提供符合文件系统层次结构的路径名，处理路径名递归查找。

文件描述符层File Descriptor Layer，最后，文件描述符是对底层所有资源（管道、设备、普通文件等）的抽象，用户对文件系统的视图是简单而统一的，这方便了用户程序的编程。

 

xv6的块大小为1kb，但常见是4kb

![img](assets\clip_image004-1729429014544-33.jpg)下面的数字表示块号，如块0表示引导块，块1表示超级块包含了关于完整文件系统的元数据，如文件系统的大小或块数、数据块的数量、inodes的数量、日志块的数量等；从块32开始是inodes，每一块inode block上面都有多个inodes（像上12页那样）；紧接着，块45的单独一块用作位图，它用于追踪哪些数据块已被分配或处于空闲；从块46开始，后续的全部用作数据块，包含着文件或目录。

超级块中存放的是一个独立的程序，称为mkfs（make file system，位于mkfs/mkfs.c），它负责构建并挂载初始的xv6文件系统。**2～46**块被称为元数据块，包含很多信息。



#### 5.1 buffer cache layer

Buffer Cache主要做两件事：

同步所有对磁盘块的并发访问，尤其是保证，一个磁盘块要么没有被缓存，要么在内存中只有一份缓存副本，而且一次只有一个内核线程可以使用该副本。

缓存磁盘块时，应该保留那些常被访问的磁盘块不被逐出Buffer Cache，这样后续的访问就可以直接访问内存中的副本，而不是通过慢速的磁盘I/O。

Buffer Cache主要向上提供两个重要接口：bread和bwrite。bread获取内存中的一份缓存块，该缓存块上包含了相应磁盘块的副本，因此我们可以读取或修改它；bwrite则将修改过后的缓存块冲刷到磁盘上，完成相应块的更新。此外，每当一个内核线程使用完一个缓存块后，应该对那个缓存块调用brelse，后续会说明这一点。Buffer Cache对每一块缓存块，都维护了一把睡眠锁，以保证一次只有一个内核线程可以使用该缓存块，因此一次只有一个内核线程在修改某一磁盘块的副本。bread返回的是一个已上锁的缓存块，而brelse释放该缓存块的锁。

 

Buffer Cache有固定数量的槽位，每个槽位保存一个磁盘块的副本。如果文件系统希望访问的磁盘块不在Buffer Cache中，就需要回收一个已经含有缓存块的槽位，以满足此次请求。Buffer Cache使用LRU算法来回收这些槽位。

struct buf每个槽位（或者说每个缓存块）对应一把睡眠锁，保护buf中的信息。其中比较重要的有valid，表示该槽位上是否缓存了磁盘块的副本；disk表示磁盘正在处理该缓存块的读或写请求，如果磁盘还未处理完成，disk=1，如果磁盘处理完成，disk=0；blockno表示缓存的磁盘块在磁盘上的位置；**refcnt****表示当前有多少个内核线程在等待这一缓存块**（因为一次只能有一个线程访问该缓存块）。

 

每个磁盘块最多被缓存一次，而且只能有一个内核线程正在访问该缓存块。这是为了保证所有的bread可以看到bwrite的相应修改。bget通过在返回缓存块b之前，一直持有bcache.lock来保证这一点，这样，检查缓存是否命中，以及未命中时对于被回收槽位buf的一系列元数据更新，这些操作都是原子的。

 

bpin和bunpin分别对缓存块的引用计数refcnt进行加减，稍后我们将看到，在日志层中可以利用这两个接口，保证要写入日志的缓存块不会被回收。

 

// 计算每个BLOCK可以包含多少个bitmap位

// BLOCK SIZE = 1024（xv6是1kb），所以一个BLOCK可以包含8192个bit

// 即BPB = 8192

\#define BPB      (BSIZE*8)

 

// Block of free map containing bit for block b

// 给定block number b，b/BPB表示该块的bitmap位应该在哪个bitmap block上

// 再加上bitmap区域的起始位置，得到最终的bitmap block number

\#define BBLOCK(b, sb) ((b)/BPB + sb.bmapstart)

balloc负责分配一块新的磁盘块，它会扫描所有的块，从块0一直到文件系统的最大块号为止。它将会找到一个空闲的磁盘块，该块在位图上标记为0，然后更新位图并返回该磁盘块。balloc有两层循环，外层循环遍历每个位图块，而内层循环则对于给定位图块，检查里面的8192位（1024*8B），直到发现一个标记为0的位，于是就决定分配那一块磁盘块，更新位图之后，返回该新分配磁盘块的块号。（就像大大块中遍历其中的8192个小小块，看哪一块为0）你可能担心有多个线程同时尝试调用balloc，是否会发生竞争条件？答案是不必担心，因为Buffer Cache有一把锁，所以调用bread时内含同步机制。

**balloc****和bfree****应该在一个事务中被调用。**

 

#### 5.2 inode layer

xv6的inode有两个含义，一个是指磁盘上的数据结构，另一个是指内存中的数据结构。磁盘上的inode包含了文件大小、数据块的位置等基本信息；而内存中的inode是前者的一份副本，而且还包含有内核所需的额外信息。

struct dinode是磁盘上inode的数据结构定义。type指示了该inode的类型，是文件、目录、设备还是未被使用；nlink表示有多少个目录条目包含这一inode。对于文件来说，nlink既是链接计数也是硬链接数，而对于目录来说，nlink只是链接计数，nlink指示这个inode及其相应的数据块什么时候被释放；size表示以字节为单位表示该文件有多大；最后addrs数组则指向inode的数据块所在位置，每个数组条目都包含了一个数据块块号。（addrs数组在下文解释）

 

struct inode则是内核在内存中维护的，正被使用的on-disk inode的一份副本。仅当存在指针指向内存中的某一inode时，内核才会保持该inode的内存副本。ref指示了有多少C指针指向该inode的内存副本，如果ref为0，内核就丢弃该inode的内存副本。在后面我们将看到，iget和iput函数可以获取或释放一个inode的指针，从而影响ref的计数值。

 


**nlink是硬链接，断电之后还会保存在磁盘里面，而ref是内存里面的数据结构，断电之后就会消失。ref是内存指针计数，只要\**ref大于0，内核就会将该inode的副本保留在icache（inode cache类似于buffer cache）中；而nlink是硬链接计数，只要nlink大于0，文件系统就不会释放该inode及其数据块在磁盘上占用的空间。ref=0本质上只是icache不再缓存该inode，并不是释放了磁盘上的inode，之后还可以再次调用iget缓存该inode**

 

多个进程可以同时拥有指向同一个inode的指针，但是通过ilock，只有一个进程可以上锁并访问该inode。设计icache的主要目的是为了同步多个进程对inode的访问（icache没有睡眠锁，所以可以并发访问该inode，这能正常工作依赖于下层的buffer cache），而利用缓存的高速性只是次要目的。如果一个inode经常被使用，就算它不在icache中，Buffer Cache也可能会缓存它。icache是直写式（write-through）的，这意味着一旦icache中的某个inode副本被修改，就要马上调用iupdate来更新到磁盘上。

 

注意ialloc和iget的区别

 // ialloc每次调用时，都会找到一个磁盘上空闲的inode块(数据结构是dinode)

 // 然后将其标记为已分配，分配完之后就会马上调用iget并将其缓存在icache中

 // iget则是给定inode number，先看对应的inode是否已经被缓存了

 // 如果没有，就分配缓存区中第一个空位来缓存该inode

 

![img](assets\clip_image002-1729429053161-36.jpg)![img](assets\clip_image004-1729429053161-37.jpg)

与iget相对应的iput，iput接收一个inode指针输入，并且将其ref计数减1。如果该指针是最后一个对该inode的引用，而且该inode硬链接数nlink为0，那么就释放磁盘上的对应inode和相关数据块。iput通过调用itrunc（在下文解释)来释放inode，将长度截断为0并且释放相关数据块，然后标记type为空闲，最后调用iupdate将新的inode元数据写回磁盘上。

 

调用iput的线程持有的那个指针，是指向该inode缓存副本的最后一个指针。要么ialloc发现该inode已被分配而跳过它；要么ialloc在iput调用iupdate之后，才发现其type=0而选中它。

 

当一个文件的硬链接数nlink为0，iput并不会马上就删除并释放该文件，因为在内存中可能还存在其它指向该文件的inode指针，甚至有进程仍然在读写这个文件。如果在最后一个进程关闭该文件的文件描述符，释放最后一个inode指针之前，崩溃或者断电发生了，下次启动时该文件仍会被标记为分配（因为没有被删除并释放），但是没有任何目录条目会包含它，也不会有任何内存中的inode指针指向它，我们失去了一些磁盘空间。

文件系统有两种方式处理这种情况：一个简单的方法是，在系统启动的恢复阶段，文件系统扫描磁盘上的整个文件系统，检查是否有文件标记被分配，但是没有任何目录条目包含它，然后如果有，就删除释放这些文件。这个思路是前两章中提到的fsck程序。

第二个方法是不需要扫描整个文件系统，而是在磁盘的某个位置上（例如超级块中），记录硬链接数为0，而指向其的指针数不为0的文件的inode号。如果该文件最后确实被删除释放了，那么就从该记录列表中移出相应的inode号。然后在系统启动的恢复阶段，不扫描整个文件系统，而是简单地扫描该列表，删除释放该列表中的文件即可。

但是，xv6文件系统并不提供以上两种的任一一种实现，这意味着随着xv6的运行，会有越来越多的磁盘空间流失，最终，我们可能会耗尽磁盘上的空间。

 

每个⽂件或⽬录在其所在⽂件系统中都有⼀个唯⼀的 inode 号。

 

硬链接提供了⼀种⾼效管理和访问⽂件的⽅法，允许多个⽂件名指向同⼀块数据。

 

Linux ⽂件系统中常⻅的做法是分别维护内存中的 inode 结构和磁盘上的 inode 结构，两者的设计和功能有所不同，但紧密相关：

 

内存中的 inode 结构（struct inode）

内存中的 `struct inode` 是 Linux 内核中的⼀个核⼼数据结构，⽤于表示⽂件系统中的⼀个⽂件或⽬录。这个结构体在内存中存储着该⽂件或⽬录的所有重要状态信息，以及必要的操作⽅法。它主要包含：

\-   **⽂件类型、权限和⼤⼩**：标识⽂件的基本属性。

\-   **链接计数 (`i_nlink`)**：表示有多少⽂件系统⼊⼝点（如硬链接）指向此 inode。

\-   **锁定机制**：如信号量或⾃旋锁，⽤于在多线程环境中保护 inode 数据。

\-   **时间戳**：如最后访问时间、修改时间等。

\-   **⽂件操作**：⼀个指向⽂件操作函数表的指针，这些函数表定义了如何读写⽂件数据。

\-   **块映射⽅法**：处理⽂件数据块到物理磁盘块映射的⽅法。

\-   **指向磁盘 inode 数据的指针**：通常是⽂件系统特有的磁盘 inode 结构。

 

磁盘上的 inode 结构（例如 struct ext4_inode）

磁盘上的 inode 结构体是⽂件系统定义的，⽤于持久存储在磁盘上的格式。这个结构体包含了在磁盘上存储⽂件或⽬录时需要保存的所有信息，⽐如：

\-   **⽂件类型和设备号**：对于设备⽂件，存储⼤、⼩设备号。

\-   **⽂件⼤⼩**：⽂件的字节⼤⼩。

\-   **数据块指针**：⽂件内容存储位置的直接、间接块指针。

\-   **链接计数**：硬链接数，⽤来管理⽂件的⽣命周期。

\-   **权限和所有权信息**：⽂件的访问权限和所有者信息。

 

关系和同步

内存中的 inode 结构体在⽂件被加载时从磁盘上的 inode 结构读取，当⽂件系统进⾏数据写⼊操作时，会定期将内存中的 inode 状态同步回磁盘上的 inode。这种设计允许系统在保证数据⼀致性的同时，提⾼⽂件访问的效率。

这种内存与磁盘结构的分离设计是⽂件系统性能优化的关键，它使得⽂件系统可以缓存频繁访问的数据，减少对磁盘的直接操作，同时保持数据的持久化和⼀致性。

 

再强调⼀次xv6 ⼀块是1kb。

 

给定inode指针，bmap就为我们找到⽂件的第bn块对应的数据块块号，这为很多更⾼层的接⼝如 readi 和writei 都提供了便利。如果该数据块还未被分配，bmap还会调⽤balloc为我们分配它。注意，这⾥只将新的⼀级间接块写⼊⽇志更新，如果分配了新的数据块导致inode的addrs[ ]发⽣了改变，那么应该由上层的调⽤来调⽤iupdate，writei 就是这么做的。（下面这个图a是在**磁盘**中的数据结构，5.1标题的前一张图b也是在磁盘上，图b展示了图a中的inode这一部分的数据结构,应该看struct dinode这个结构体，struct inode是内存中对dinode的backup用于高速访问，一个inode64B，这64B就是由type（short 2字节）nlink size等等加起来得到。比如说我们需要读取文件的第8000个字节，那么用8000除以1024可以发现它在直接块中的的第七块，然后用8000%1024=832就得出了偏移量。可见（12+256）*1024B=268kb，这就是xv6所能存下的大小，12个直接块256个间接块）

 

itrunc释放⼀个给定inode的所有数据块，并且将inode⻓度截断为0。同样地，先检查直接块，然后再检查间接块和间接数据块。

![img](assets\clip_image006-1729429053161-38.jpg)

 

上面所有东西存在inode中，总共64B

来看两个位于bmap之上的接⼝，readi 和writei，看bmap 是如何被利⽤的。

readi 如下，开始时先检查读取是否合法，不能读超过⽂件⼤⼩的字节。然后主循环就会不断地读出⽂件相应的数据块，并把数据块的内容从Buffer Cache 拷⻉到dst 中。writei 如下，和readi 的架构相 似，但有⼏个不同之处：⼀是writei 可以超出⽂件⼤⼩，从⽽增⻓⽂件；⼆是与readi 相反，writei 拷

⻉数据到Buffer Cache 中；三是⼀旦⽂件增⻓，就要更新其inode的⼤⼩信息。值得⼀提的是，即便⽂件⼤⼩没有增⻓，也照样调⽤iupdate将inode写⼊磁盘，因为在调⽤bmap时可能分配了新的数据块，从⽽inode的addrs[ ]会被改变。stati将inode的元数据拷贝到位于内存中的struct stat，为上层的用户进程提供访问该inode元数据的接口。





#### 5.3 directory layer

⽬录实质上也是⼀种⽂件，它的inode类型字段是T_DIR，其数据内容是⼀系列⽬录条⽬。每个⽬录条

⽬的类型是struct dirent，每个条⽬包含了⼀个⽤户可读名称和inode号。⽤户可读名称最⻓为14 个字符，⽽inode号为0 的⽬录条⽬是空闲的。

 

dirlookup给定⼀个⽬录的inode指针和条⽬名称，查找相应的⽬录条⽬。如果找到，就返回该⽬录条

⽬的inode指针，且将*poff 设为该条⽬在⽬录中的偏移量。如果没有则返回0。

 

iget不给相应的inode上锁的原因是，避免在路径名查找时发⽣死锁。因为调⽤dirlookup的线程已经对当前⽬录dp上锁了，⽽如果要查找的路径名是"."，即当前⽬录，那么就会发⽣死锁，多线程并发和查找上⼀级⽬录".."也可能会导致死锁。因此我们规定，调⽤线程⼀次只能持有⼀把锁，它应该在 dirlookup返回之后，解锁dp，⽽对新的inode上锁。

 

dirlink则给定⼀个条⽬名称和相应的inode号，在给定⽬录dp中创建⼀个新的⽬录条⽬，即创建⼀个新的硬链接。如果该条⽬已经存在，dirlink返回-1 表示错误；在主循环中，查找⼀个空的位置写⼊新的条⽬，如果当前位置已满，那么dp的size 会增加（通过writei）。



#### 5.4 pathname layer

前⾯简单说明了，给定⼀个⽬录，如何在其中查找或添加相应条⽬。（dirlookup dirlink）现在我们关注完整的路径名查找过程，这个过程需要调⽤⼀连串的dirlookup，每个⽬录名都需要⼀次。

 

namei和nameiparent就负责这样的路径名查找，它们都接收完整的路径名作为输⼊，返回相关的 inode。不同之处在于，namei返回路径名中最后⼀个元素的inode；⽽nameiparent返回最后⼀个元素的⽗⽬录的inode，并且将最后⼀个元素的名称复制到调⽤者指定的位置*name 中。这两个函数都调

⽤namex完成路径名查找的⼯作。

 

在分析namex之前，我们先分析namex⽤来解析路径名的skipelem，如下所示。skipelem做的事情是解析给定的路径名path。它提取出path 中的下⼀个元素，并拷⻉到name中，然后返回在下个元素之后的后续路径，如注释中的⼏个示例所示。如果提取出的下⼀个元素已经是最后⼀个元素，那么返回"\0"。如果输⼊path 是"\0"，那么返回0。

 

Examples:

//   skipelem("a/bb/c", name) = "bb/c", setting name = "a"

//   skipelem("///a//bb", name) = "bb", setting name = "a"

//   skipelem("a", name) = "", setting name = "a"

//   skipelem("", name) = skipelem("////", name) = 0

// ⽬标是将路径中的下⼀个元素（即路径的下⼀部分直到下⼀个斜杠或字符串结束，会跳过⽆效的斜杠⽐如第⼆个例⼦中的斜杠）提取到 name 中，并返回剩余的路径部分。

 

namex如下所示，整个流程⽐较⻓⽽复杂，我们⼀步步解析它。⾸先，namex决定从哪⾥开始查找路径，如果路径名path 以"/"开头，那么就从根⽬录开始；否则从当前⼯作⽬录开始。决定了从哪⾥开始之后，namex就使⽤skipelem来解析当前路径名path。

 

static struct inode*

namex(char *path, int nameiparent, char *name)

{

 struct inode *ip, *next;

 

 // 决定从哪里开始找，如果有'/'就从根目录开始，否则从当前目录开始

 if(*path == '/')

  ip = iget(ROOTDEV, ROOTINO);

 else

  ip = idup(myproc()->cwd);

 

 // uses skipelem to consider each element of the path in turn

 while((path = skipelem(path, name)) != 0){

/ /skipelem将下个path element拷贝到name中，返回跟在下个path element的后续路径

  // 如果*path=='\0'就表示提取出的path element已经是最后一个

  ilock(ip);

  // 检查当前结点ip是否为目录类型

  if(ip->type != T_DIR){

   iunlockput(ip);

   return 0;

  }

  if(nameiparent && *path == '\0'){

   // path='\0'表示name已经是最后一个元素

   iunlock(ip);

   // 调用者为nameiparent，当前ip就是最后一个元素的父目录

   // 最后一个元素的名称在skipelem中已被复制到name

   return ip;

  }

  // 在当前目录结点ip下，找name，也就是找下一个path element

  if((next = dirlookup(ip, name, 0)) == 0){

   // 在当前目录结点下找不到

   iunlockput(ip);

   return 0;

  }

  iunlockput(ip);

  // 释放ip，为下一次循环做准备

  // 当查找'.'时，next和ip相同，若在释放ip的锁之前给next上锁就会发生死锁

  // 因此namex在下一个循环中获取next的锁之前，在这里要先释放ip的锁，从而避免死锁

  ip = next;

 }

 if(nameiparent){

  // 正常情况下nameiparent应该在主循环中就返回

  // 如果运行到了这里，说明nameiparent失败，因此namex返回0

  iput(ip);

  return 0;

 }

 // When the loop runs out of path elements, it returns

 return ip;

}

以这个为例子：  

  // 例: 最开始path = '/a/b'，b可以是文件也可以是目录，当前目录ip为根目录'/'

  // 1、path = /b,name = a,next = name = a,ip = next = a

  // 2、path = "\0",name = b,(如果是nameiparent就在这一步停止并返回ip = a,name = b),

  // next = name = b,ip = next = b

  // 3、path = 0,name = b,ip = b,namei在跳出循环后返回ip = b

namex的主循环工作流程如下：

 

1  使用skipelem解析路径名，将下一个元素放入name中，并更新path为跟在下一个元素之后的后续路径，可以看在代码注释中举的例子。

2  如果path返回0，说明路径遍历已经结束，则跳出循环，在末尾返回最终的inode。值得注意的是，namex的上层调用是namei时，跳出循环并在末尾返回ip才是正确的；nameiparent应该在主循环中就结束其工作并返回，如果它也跳出循环，只能说明nameiparent失败。

3  主循环中的第一步，给当前目录结点ip上锁，然后检查ip是否为目录类型，如果不是，整个namex就会失败。我们要给ip上锁的原因是，直到我们调用ilock为止，不保证ip的ip->type已经从磁盘上读出。

4   接着，主循环中检查上层调用是否来自nameiparent。如果是，而且path='\0'（回忆skipelem，这表示name里面是最后一个元素），那么nameiparent应该就此结束，此时ip刚好就是最后一个元素的父目录，因此返回ip。

5   然后，主循环调用dirlookup查找当前目录下的目录条目，在ip中查找name，即查找ip的下一个元素并保存到next中。如果当前目录下没有这个条目，namex也会失败。

6   最后，主循环释放ip的锁，然后更新ip为next。next是目录或文件，如果是目录，下一个循环还能继续；如果是文件，那么将跳出下一次循环。值得注意的是，在这里我们先释放了ip的锁，然后在下一次循环开始时，再获取next的锁。这是为了避免死锁，例如当查找"."时，next和ip相同，若在释放ip的锁之前给next上锁，就会发生死锁。

 

当一个调用namex的内核线程阻塞在磁盘I/O上时，另一个对不同路径名进行查找的内核线程可以执行namex。这是因为namex的设计很谨慎，它分别对路径名中的每个目录上锁，再调用dirlookup查找该目录。因此，就算有多个内核线程调用namex，只要dirlookup查找的目录是不同的，我们就可以并发地进行路径名查找。当然，这种并发可能会带来一些挑战。例如，一个内核线程在查找路径名的同时，另一个内核线程可能正在删除某个目录。可能存在的风险是，一个内核线程的dirlookup正在查找一个目录，但该目录已被另一个内核线程删除，而且原来的数据块也被分配给了新的目录或文件。

 

xv6避免了这种情况发生，因为在执行namex的dirlookup时，我们拥有当前目录ip的锁，然后dirlookup返回一个inode指针给我们，这个inode指针是通过iget得到的，iget会增加inode的ref计数。因此，如果另一个内核线程对该inode调用iput，该inode是不会被删除的，因为企图删除inode的内核线程并不是最后一个指针的拥有者。

 

在namex中，严格遵循先释放ip的锁，再获取next的锁的规则。（因为当dirlookup查找的是"."时，next和ip是相同的。因此在释放ip的锁之前就对next上锁，就有引起死锁的风险。）



#### 5.5 File Descriptor layer

**UNIX****操作系统实现了很好的抽象——****一切皆文件，无论底层是普通文件、目录、还是控制台、管道等设备，用户都可以将它们看成是文件。**

一般每个进程都有一个打开文件表，有一系列相应的文件描述符。每次调用open时，都会创建一个新的打开文件，即创建一个新的struct file。因此，如果有多个进程同时打开同一个文件，它们的struct file实例是不同的，即它们会有不同的偏移量。另一方面，相同的struct file实例，可以多次出现在一个或多个进程的打开文件表中，例如使用open打开文件之后，又调用了dup或fork等系统调用时，就会出现这种情况。ref字段跟踪该struct file被引用了多少次，还有两个字段指示该打开文件是否允许读或写。

![img](assets\clip_image002-1729429189245-42.jpg)xv6使用了一个全局的打开文件表，称为ftable，系统中所有的打开文件都会存放在这个ftable中。此外，每个xv6进程都有一个打开文件表ofile，每个用户进程最多同时打开16个文件，即用户进程的文件描述符号范围是从0到15。

 

filealloc分配一个新的打开文件struct file，它扫描ftable，找到第一个空闲的槽位，然后返回新的引用。打开文件数达到上限时，filealloc只是返回0，并不会使xv6陷入panic。

filedup创建一个打开文件struct file的副本，逻辑很简单，只是简单地增加引用计数ref，然后返回指向相同struct file的指针。

fileclose减少ref引用计数。当ref大于1时，直接将ref减1即可返回；当ref=1时，代表对该打开文件的最后一个引用也将删除，fileclose就更改该打开文件的类型为FD_NONE，然后再检查该打开文件下层资源的类型，如果是pipe或inode，还要相应地调用pipeclose或iput，以关闭和释放这些底层资源。

filestat利用前一章中的stati接口，为系统调用fstat提供服务。filestat只允许对inode类型读取其stat信息。

fileread根据不同的底层文件类型，检查文件可读模式是否打开，然后调用不同的方法（比如管道和真正的文件的方法就是不同的）来读取这些资源，为系统调用read提供服务。fileread和接下来的filewrite都使用了struct file中的偏移量off，每次读完之后就更新它，有一个例外是管道，管道没有偏移量。

filewrite和fileread类似，这次检查写模式是否打开，它为系统调用write提供服务。fileread和filewrite中利用了ilock的锁机制，读取和写入的偏移量都将会原子地更新，因此对同一文件的多次写入不会覆盖彼此的数据，对同一文件的多次读也不会读出重复的数据。利用ilock的锁机制，我们不需要在struct file中再额外添加一把锁保护off。

fdalloc根据给定的ftable中的打开文件f，在用户进程的打开文件表ofile中记录f，并且分配一个文件描述符。

 

sys_link为给定的inode创建新的目录条目，即创建新的硬链接。首先sys_link从寄存器获取参数，a0是旧路径名，a1是新路径名。如果旧路径名存在而且不是目录（目录不能创建硬链接），那么就使该inode的硬链接数加1。然后，sys_link调用nameiparent来查找新路径名的最后一个元素的父目录，然后在该目录中，创建一个新的目录条目。限制条件是，新路径名必须存在，而且要与旧路径名位于同一个设备上，因为inode号只在同一个设备上有效。



**如果你想使用硬链接，你需要为链接指定一个与原文件不同的名字或路径。但它们都是指向的同一个文件。**

**/**

**├── home/**

**│  ├── alice/**

**│  │  ├── file.txt**

**│  └── bob/**

**ln /home/alice/file.txt /home/bob/link.txt与ln /home/alice/file.txt /home/alice/link.txt都是可以的，ln /home/alice/file.txt /home/alice/file.txt就不行，因为实际上是在尝试让一个文件指向其自身，这是不行的。**

**以ln /home/alice/file.txt /home/alice/link.txt为例子，解释此系统调用：**

![img](assets\clip_image002-1729429201340-44.jpg)

**事务通常指的是一系列操作，这些操作要么全部成功，要么全部失败，它们作为一个整体执行以确保数据的完整性和一致性。**

 

sys_link为一个已存在的inode创建新的硬链接，而create则根据给定的路径名，创建一个全新的inode，并且创建硬链接，如下所示。有三种情况会用到create：open使用O_CREATE模式创建一个新文件；mkdir创建一个新目录；mkdev创建一个新设备文件。

![img](assets\clip_image004-1729429201340-45.jpg)![img](assets\clip_image006-1729429201340-46.jpg)

**第五点是错的，应该直接从第4点到第6点。**



![img](assets\clip_image002-1729429256111-50.jpg)![img](assets\clip_image004-1729429256111-51.jpg)

**创建文件和创建目录的过程在几个关键步骤上有所不同，主要体现在对新 inode** **的初始化和父目录的处理上。以下是主要的差异点：**

 

inode 类型和分配:

文件：在创建文件时，新 inode 被分配为类型 T_FILE。

目录：在创建目录时，新 inode 被分配为类型 T_DIR。目录的处理需要额外的步骤来初始化目录结构。

 

 

目录项的初始化:

文件：创建文件时不需要添加任何特别的目录项，因为文件本身不包含子目录项。

目录：创建目录时必须初始化 . 和 .. 目录项。. 指向目录本身，而 .. 指向其父目录。这两个条目是每个目录的基本结构，非常重要，它们确保了目录结构的完整性和导航的正确性。

 

父目录的 nlink 更新:

文件：创建文件时，父目录的链接数 (nlink) 通常不会改变，因为文件不增加目录结构的链接需求。

目录：创建目录时，需要增加父目录的链接数。这是因为每个目录都有一个 .. 条目指向其父目录，这相当于在父目录中添加了一个新的链接。

 

循环引用计数的处理:

文件：创建文件时，不涉及到循环引用计数问题。

目录：在创建目录时，特别注意不为 . 增加 nlink。这样做是为了避免目录对自己的引用形成循环，这可以保证当目录被删除时，其引用计数能够正确地减到零。

 

利用create，很容易就能实现上层的sys_open、sys_mkdir和sys_mknod等系统调用接口。这里我们来看最复杂的sys_open，如果调用open时设置了O_CREATE模式，那么将调用create；否则，open默认打开一个现有的文件，因此调用namei。create返回上锁的inode，而namei返回的inode并不上锁，所以还需要显式地调用ilock。无论ip是由create新分配的，还是用namei打开的原有的，sys_open接着调用filealloc分配一个打开文件，并且调用fdalloc分配一个文件描述符，接着填充struct file结构。

 

最后再看sys_pipe，它是创建（匿名）管道的系统调用，调用时，我们要传入一个容纳两个整型的数组作为参数，接着调用pipealloc创建一个新的管道，管道两端的打开文件结构rf和wf也会被创建并设置好。然后sys_pipe通过fdalloc，用rf和wf分配两个新的文件描述符，最后通过copyout在传入的数组中放置这两个新的文件描述符fd0和fd1。



## 课上内容



![img](assets\clip_image002-1729429293993-54.jpg)

API（应用程序编程接口)是一组规则和定义，允许一个软件应用或系统与另一个软件应用或系统进行交互。API定义了请求数据的方式，如何进行请求，以及响应的类型，让不同的软件之间能够相互操作。

 

简单的例子：假设你在使用一个天气预报应用。这个应用程序通过调用一个天气服务的API来获取当前的天气信息。当你打开应用查看天气时，应用向天气服务发送一个请求，天气服务的API处理这个请求，然后将天气数据发送回应用。这样，你就可以在应用界面上看到最新的天气情况。这个过程就是通过API实现的交互。

 

在文件系统内部，文件描述符必然与某个对象关联，而这个对象不依赖文件名。这样，即使文件名变化了，文件描述符仍然能够指向或者引用相同的文件对象。所以，实际上操作系统内部需要对于文件有内部的表现形式，并且这种表现形式与文件名无关。文件系统的目的是实现上面描述的API，也即是典型的文件系统API。但是，这并不是唯一构建一个存储系统的方式。如果只是在磁盘上存储数据，你可以想出一个完全不同的API。举个例子，数据库也能持久化的存储数据，但是数据库就提供了一个与文件系统完全不一样的API。所以记住这一点很重要：还存在其他的方式能组织存储系统。

文件系统 API 是一组用于操作文件和目录的程序接口，允许软件应用程序通过标准化的命令与底层的文件系统交互。这些 API 提供了创建、读取、写入和删除文件或目录的功能，以及查询和修改文件属性等操作。

例如，如果你正在使用一个文本编辑器，当你打开、保存或修改文件时，编辑器将调用文件系统 API 来在磁盘上读取或写入数据。这些 API 确保了编辑器可以与任何遵循相同标准的文件系统兼容，无论是在 Windows、macOS 还是 Linux 操作系统上。

简单来说，文件系统 API 就像是应用程序与文件系统之间的翻译器，帮助应用程序以一种统一的方式处理文件，而无需关心文件实际存储在硬盘上的具体细节。这使得开发者可以更容易地编写能够跨平台工作的应用程序。它实现于操作系统上，为用户层提供了对文件系统的高度抽象。

 

一个文件（inode）只能在link count为0的时候被删除。实际的过程可能会更加复杂，实际中还有一个openfd count，也就是当前打开了文件的文件描述符计数。一个文件只能在这两个计数器都为0的时候才能被删除。

 

**文件系统中核心的数据结构就是inode和file descriptor（第四层和第七层）。后者主要与用户进程进行交互。**

 

在最底层是磁盘，也就是一些实际保存数据的存储设备，正是这些设备提供了持久化存储。（这里是device，下面都是memory）

在这之上是buffer cache或者说block cache，这些cache可以避免频繁的读写磁盘。这里我们将磁盘中的数据保存在了内存中。

为了保证持久性，再往上通常会有一个logging层。许多文件系统都有某种形式的logging，我们下节课会讨论这部分内容，所以今天我就跳过它的介绍。

在logging层之上，XV6有inode cache，这主要是为了同步（synchronization），我们稍后会介绍。inode通常小于一个disk block，所以多个inode通常会打包存储在一个disk block中。为了向单个inode提供同步操作，XV6维护了inode cache。

再往上就是inode本身了。它实现了read/write。(inode cache 和inode组成那第四层）

再往上，就是文件名，和文件描述符操作。

 

 

![img](assets\clip_image004-1729429293993-55.jpg)![img](assets\clip_image006-1729429293993-56.jpg)

bitmap block，inode blocks和log blocks被统称为metadata block。它们虽然不存储实际的数据，但是它们存储了能帮助文件系统完成工作的元数据。boot block包含了操作系统启动的代码。

![img](assets\clip_image008-1729429293993-57.jpg)

看一下**磁盘上存储的****inode**究竟是什么？首先我们前面已经看过了，这是一个64字节的数据结构。

通常来说它有一个type字段，表明inode是文件还是目录。

nlink字段，也就是link计数器，用来跟踪究竟有多少文件名指向了当前的inode。

size字段，表明了文件数据有多少个字节。

不同文件系统中的表达方式可能不一样，不过在XV6中接下来是一些block的编号，例如编号0，编号1，等等。XV6的inode中总共有12个block编号。这些被称为direct block number。这12个block编号指向了构成文件的前12个block。举个例子，如果文件只有2个字节，那么只会有一个block编号0，它包含的数字是磁盘上文件前2个字节的block的位置。

 

之后还有一个indirect block number，它对应了磁盘上一个block，这个block包含了256个block number，这256个block number包含了文件的数据。所以inode中block number 0到block number 11都是direct block number，而block number 12保存的indirect block number指向了另一个block。因为是uint型，然后xv6一个block是1kb，所以1024/4=256个block编号，加上前面12个直接块为268块。

![img](assets\clip_image010-1729429293993-58.jpg)对于实现路径名查找，这里的信息就足够了。假设我们要查找路径名“/y/x”，我们该怎么做呢？

 

从路径名我们知道，应该从root inode开始查找。通常root inode会有固定的inode编号，在XV6中，这个编号是1。我们该如何根据编号找到root inode呢？从前一节我们可以知道，inode从block 32开始，如果是inode1，那么必然在block 32中的64到128字节的位置。所以文件系统可以直接读到root inode的内容。

 

对于路径名查找程序，接下来就是扫描root inode包含的所有block，以找到“y”。该怎么找到root inode所有对应的block呢？根据前一节的内容就是读取所有的direct block number和indirect block number。结果可能是找到了，也可能是没有找到。如果找到了，那么目录y也会有一个inode编号，假设是251，我们可以继续从inode 251查找，先读取inode 251的内容，之后再扫描inode所有对应的block，找到“x”并得到文件x对应的inode编号，最后将其作为路径名查找的结果返回。

 

XV6 总是会打印⽂件系统的⼀些信息，所以从指令的下⽅可以看出有46 个meta block，其中包括了：

●boot block

●super block

●30 个log block

●13 个inode block

●1 个bitmap block

之后是954 个data block。

⼀个block 有1kb，⼀个inode64B，所以应该block 有64 个inode，总共13 个block，所以有208 个 inode，如果每个都指满了（即12 个直接块加1 个间接块即总共268 个块），总共该有208*268 个块为55744 个块，但它才指了U54 个块，说明没指满。正常情况下，不同的inode 不会指向相同的数据块，除⾮涉及硬链接这种特殊情况。在硬链接中，多个⽬录项会共享⼀个inode，从⽽共享同⼀组数据块。

 

接下来我们运⾏⼀些命令，来看⼀下特定的命令对哪些block 做了写操作，并理解为什么要对这些 block 写⼊数据。我们通过echo “hi” > x，来创建⼀个⽂件x，并写⼊字符“hi”。我会将输出拷⻉出来，并做分隔以⽅便我们更好的理解。

![img](assets\clip_image012-1729429293993-60.jpg)

 

这⾥会有⼏个阶段

●   第⼀阶段是创建⽂件  

第⼆阶段将“hi”写⼊⽂件

第三阶段将“\n”换⾏符写⼊到⽂件



让我们⼀个阶段⼀个阶段的看echo 的执⾏过程，并理解对于⽂件系统发⽣了什么。相⽐看代码，这⾥直接看磁盘的分布图更⽅便：

![img](assets\clip_image014-1729429293993-59.jpg)

 

你们觉得的write 33 代表了什么？我们正在创建⽂件，所以我们期望⽂件系统⼲什么呢？学⽣回答：这是在写inode。

是的，看起来给我们分配的inode位于block 33。之所以有两个write 33，第⼀个是为了标记inode将要被使⽤。在XVG 中，我记得是使⽤inode中的type字段来标识inode是否空闲，这个字段同时也会

⽤来表示inode是⼀个⽂件还是⼀个⽬录。所以这⾥将inode的type从空闲改成了⽂件，并写⼊磁盘表示这个inode已经被使⽤了。第⼆个write 33 就是实际的写⼊inode的内容。inode的内容会包含 linkcount 为1 以及其他内容。

write 46 是向第⼀个data block写数据，那么这个data block属于谁呢？回答：属于根⽬录。

是的，block 46 是根⽬录的第⼀个block。为什么它需要被写⼊数据呢？学⽣回答：因为我们正在向根⽬录创建⼀个新⽂件。

是的，这⾥我们向根⽬录增加了⼀个新的entry，其中包含了⽂件名x，以及我们刚刚分配的inode编号。

接下来的write 32 ⼜是什么意思呢？block 32 保存的仍然是inode，那么inode中的什么发⽣了变化使得需要将更新后的inode写⼊磁盘？是的，根⽬录的⼤⼩变了，因为我们刚刚添加了16 个字节的entry来代表⽂件x的信息。

最后⼜有⼀次write 33，我在稍后会介绍这次写⼊的内容，这⾥我们再次更新了⽂件x的inode， 尽管我们⼜还没有写⼊任何数据。

 

以上是第⼀阶段创建⽂件的过程。第⼆阶段是向⽂件写⼊“hi”。

 

⾸先是write 45，这是更新bitmap。⽂件系统⾸先会扫描bitmap来找到⼀个还没有使⽤的data block，未被使⽤的data block对应bit 0。找到之后，⽂件系统需要将该bit 设置为1，表示对应的 data block已经被使⽤了。所以更新block 45 是为了更新bitmap。

接下来的两次write 595 表明，⽂件系统挑选了data block 595。所以在⽂件x的inode中，第⼀个 direct block number 是595。因为写⼊了两个字符，所以write 595 被调⽤了两次。

第⼆阶段最后的write 33 是更新⽂件x对应的inode中的size 字段，因为现在⽂件x中有了两个字符。

 

接下来我们通过查看XVG 中的代码，更进⼀步的了解⽂件系统。因为我们前⾯已经分配了inode，我们先来看⼀下这是如何发⽣的。sysfile.c 中包含了所有与⽂件系统相关的函数，分配inode发⽣在 sys_open 函数中，这个函数会负责创建⽂件。

 ![img](assets\clip_image002-1729429489785-68.jpg)

在sys_open 函数中，会调⽤create 函数。

 ![img](assets\clip_image004-1729429489785-69.jpg)

 

create 函数中⾸先会解析路径名并找到最后⼀个⽬录，之后会查看⽂件是否存在，如果存在的话会返回错误。之后就会调⽤ialloc（inode allocate），这个函数会为⽂件x分配inode。ialloc函数位于 fs.c ⽂件中。

 

 ![img](assets\clip_image006-1729429489785-70.jpg)

以上就是ialloc函数，与XVG 中的⼤部分函数⼀样，它很简单，但是⼜不是很⾼效。它会遍历所有可能的inode编号，找到inode所在的block，再看位于block中的inode数据的type字段。如果这是⼀个空闲的inode，那么将其type字段设置为⽂件，这会将inode标记为已被分配。函数中的log_write就是我们之前看到在console 中有关写block的输出。这⾥的log_write 是我们看到的整个输出的第⼀个。

以上就是第⼀次写磁盘涉及到的函数调⽤。这⾥有个有趣的问题，如果有多个进程同时调⽤create 函数会发⽣什么？对于⼀个多核的计算机，进程可能并⾏运⾏，两个进程可能同时会调⽤到ialloc函数，然后进⽽调⽤bread（block read）函数。所以必须要有⼀些机制确保这两个进程不会互相影响。

（锁）

 

让我们看⼀下位于bio.c的buffer cache 代码。⾸先看⼀下bread函数

![img](assets\clip_image008-1729429489785-71.jpg)

bread函数⾸先会调⽤bget函数，bget会为我们从buffer cache 中找到block的缓存。让我们看⼀下 bget函数

 ![img](assets\clip_image010-1729429489785-72.jpg)

这⾥的代码还有点复杂。我猜你们之前已经看过这⾥的代码，那么这⾥的代码在⼲嘛？学⽣回答：这⾥遍历了linked-list，来看看现有的cache 是否符合要找的block。

是的，我们这⾥看⼀下block 33 的cache是否存在，如果存在的话，将block对象的引⽤计数

（refcnt）加1，之后再释放bcache 锁，因为现在我们已经完成了对于cache的检查并找到了block cache。之后，代码会尝试获取block cache的锁。

所以，如果有多个进程同时调⽤bget的话，其中⼀个可以获取bcache 的锁并扫描buffer cache。此时，其他进程是没有办法修改buffer cache 的（注，因为bacche 的锁被占住了）。之后，进程会查找block number 是否在cache中，如果在的话将block cache的引⽤计数加1，表明当前进程对block cache有引⽤，之后再释放bcache 的锁。如果有第⼆个进程也想扫描buffer cache，那么这时它就可以获取bcache 的锁。假设第⼆个进程也要获取block 33 的cache，那么它也会对相应的block cache的引⽤计数加1。最后这两个进程都会尝试对block 33 的block cache调⽤acquiresleep 函数。 acquiresleep 是另⼀种锁，我们称之为sleep lock，本质上来说它获取block 33 cache的锁。其中⼀个进程获取锁之后函数返回。在ialloc函数中会扫描block 33 中是否有⼀个空闲的inode。⽽另⼀个进程会在acquiresleep 中等待第⼀个进程释放锁。

学⽣提问：当⼀个block cache的refcnt 不为0 时，可以更新block cache吗？因为释放锁之后，可能会修改block cache。

Frans 教授：这⾥我想说⼏点；⾸先XVG 中对bcache 做任何修改的话，都必须持有bcache 的锁；其次对block 33 的cache做任何修改你需要持有block 33 的sleep lock。所以在任何时候， release(&bcache.lock) 之后，b->refcnt 都⼤于0。block的cache只会在refcnt 为0 的时候才会被驱逐，任何时候refcnt ⼤于0 都不会驱逐block cache。所以当b->refcnt ⼤于0 的时候，block cache本身不会被buffer cache 修改。这⾥的第⼆个锁，也就是block cache的sleep lock，是⽤来保护block cache的内容的。它确保了任何时候只有⼀个进程可以读写block cache。

如果buffer cache 中有两份block 33 的cache将会出现问题。假设⼀个进程要更新inode1U，另⼀个进程要更新inode20。如果它们都在处理block 33 的cache，并且cache有两份，那么第⼀个进程可能持有⼀份cache并先将inode1U写回到磁盘中，⽽另⼀个进程持有另⼀份cache会将inode20 写回到磁盘中，并将inode1U的更新覆盖掉。所以⼀个block只能在buffer cache 中出现⼀次。你们在完成File system lab时，必须要维持buffer cache 的这个属性。

学⽣提问：如果多个进程都在使⽤同⼀个block的cache，然后有⼀个进程在修改block，并通过强制向磁盘写数据修改了block的cache，那么其他进程会看到什么结果？

Frans 教授：如果第⼀个进程结束了对block 33 的读写操作，它会对block的cache调⽤brelse

（block cache release）函数。

 ![img](assets\clip_image012-1729429489785-73.jpg)

这个函数会对refcnt 减1，并释放sleep lock。这意味着，如果有任何⼀个其他进程正在等待使⽤这个 block cache，现在它就能获得这个block cache的sleep lock，并发现刚刚做的改动。brelease 函数中⾸先释放了sleep lock；之后获取了bcache 的锁；之后减少了block cache的引⽤计数，表明⼀个进程不再对block cache感兴趣；最后如果引⽤计数为0，那么它会修改buffer cache 的linked-list，将block cache移到linked-list 的头部，这样表示这个block cache是最近使⽤过的block cache。这

⼀点很重要，当我们在bget函数中不能找到block cache时，我们需要在buffer cache 中腾出空间来存放新的block cache，这时会使⽤LRU（Least Recent Used）算法找出最不常使⽤的block cache，并撤回它。

假设两个进程都需要分配⼀个新的inode，且新的inode都位于block 33。如果第⼀个进程分配到了 inode18 并完成了更新，那么它对于inode18 的更新是可⻅的。另⼀个进程就只能分配到inode1U，因为inode18 已经被标记为已使⽤，任何之后的进程都可以看到第⼀个进程对它的更新。

这正是我们想看到的结果，如果⼀个进程创建了⼀个inode或者创建了⼀个⽂件，之后的进程执⾏读就应该看到那个⽂件。

 

block cache代码这⾥有⼏件事情需要注意：

●   

⾸先在内存中，对于⼀个block只能有⼀份缓存。这是block  cache必须维护的特性。

●   

其次，这⾥使⽤了与之前的spinlock 略微不同的sleep lock。与spinlock 不同的是，可以在I/O

操作的过程中持有sleep lock。

●   

第三，它采⽤了LRU作为cache替换策略。

●   

第四，它有两层锁。第⼀层锁⽤来保护buffer cache 的内部数据；第⼆层锁也就是sleep lock

⽤来保护单个block的cache。

 