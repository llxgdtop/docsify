## 课前内容

缺页错误（Page Fault）是一种在虚拟内存系统中发生的中断（或异常），它发生在当一个程序尝试访问一个映射在虚拟地址空间中但尚未加载到物理内存中的内存页时。换句话说，就是当程序想要访问的数据不在物理内存（RAM）中时，操作系统就会遇到缺页错误。缺页错误Page-fault这种异常，可以被很多操作系统内核利用，从而实现写时复制Copy-On-Write方法，例如copy-on-write fork。

通过使用copy-on-write的fork，我们可以改进原来的fork。其基本思想是，在开始时，父进程和子进程共享所有的物理页，但是这些物理页全部标为只读。接着，父子进程其中一个尝试对这些物理页进行写入的时候，就会抛出一个缺页错误（Store Page Fault）。接着，对于引起异常的虚拟地址尝试访问的那页物理页，内核将其（物理内存）复制一页并映射，因此现在父子进程在各自的虚拟地址空间内都有该物理页，开放这两页的权限都为读和写，并更新两个进程的页表，之后，内核会恢复执行引起异常的那条存储指令，现在它能够正确的执行。而且COW对于进程来说是透明的。

当父进程或子进程尝试修改（写入）某个共享的物理内存页时，操作系统会捕获到这个试图写入只读页面的操作（写时缺页错误），并在这一时刻，为写入的进程在物理内存中创建所需页面的一个副本。接下来发生的是：

新复制的物理页（一个独立的内存页，现在包含了同一页面原内容的副本）会被赋予可写的权限。

要写入的进程（不管是父进程还是子进程）的虚拟地址空间中的页表会被更新，将原本指向共享物理页的虚拟地址重定向到新复制的物理页。

操作系统之后允许写入操作的执行，此时操作的是新分配的物理页。

这样，只有修改内存的那个进程会得到新的物理内存页副本，而不是两个进程都获得副本。另一个进程继续和原来一样，指向那个共享的物理页，直到它也尝试写入为止。

 

另一个利用缺页错误而实现的很棒的功能是懒惰分配Lazy Allocation，它可以分两步实现。

首先，用户进程可以调用sbrk请求更多的物理内存，这点在前一章已经展示过，只不过，现在内核只是简单地标记该进程的大小已增长（例如增加p->sz），却不急着分配物理内存给它，因此，那些新增长的虚拟地址在页表中是无效的。接着，使用这些虚拟地址就会引发缺页错误，现在内核再为该虚拟地址分配一页物理内存kalloc()，并且更新页表。

如果观察用户进程的行为，会发现它们经常向分配器请求，比它们实际上所需要的，更多的物理内存。在这种情况下，内核只在用户进程实际用到它们的时候，再来为它们分配物理页。通过这种懒惰的思想，内核尽可能地推迟物理内存的分配，可以避免很多因用户进程的贪婪而带来的不必要的分配。同样地，懒惰分配对用户进程也应该是透明的。

 

同样利用了缺页错误的，还有请求调页Demand Paging这一功能。当一个用户进程需要装载一些页到物理内存中，但RAM的空闲空间不足时，内核此时可以逐出evict一些物理页，将这些页写到诸如磁盘等地方，同时在相应页表中标记PTE为无效。之后，当别的进程读写这些被逐出的页时，就会产生缺页错误。内核发现这些页已经被换出到磁盘上，因此首先在RAM中为它们找到空间，然后从磁盘上重新读进这些页，改写PTE为有效，更新页表，并且重新执行读写指令。内核在RAM为这些物理页找空间的时候，可能又要逐出别的页。但是，整个过程对用户进程仍然是透明的。

请求调页避免了在进程运行时，直接将所有进程的物理页载入物理内存中，而是在进程需要的时候，才从磁盘上加载这些页面。这种模式，在满足访存局部性least recently used的情况下表现很好，进程在一个时间段内只访问固定一部分物理页，这避免了频繁地换入换出页。



## 课上内容

sbrk n中的n指想要分配多少页,根据Linux man page，实际中sbrk的参数是字节数.

从硬件和XV6的角度来说，当出现了page fault，现在有了3个对我们来说极其有价值的信息，分别是：

引起page fault的内存地址

引起page fault的原因类型

引起page fault时的程序计数器值，这表明了page fault在用户空间发生的位置

![img](assets\clip_image002-1729267433129-1.jpg)

### Lazy page allocation

核心思想非常简单，sbrk系统调基本上不做任何事情，唯一需要做的事情就是提升*p->sz*，将*p->sz*增加n，其中n是需要新分配的内存page数量。但是内核在这个时间点并不会分配任何物理内存。之后在某个时间点，应用程序使用到了新申请的那部分内存，这时会触发page fault，因为我们还没有将新的内存映射到page table。所以，如果我们解析一个大于旧的*p->sz*，但是又小于新的*p->sz**（注，也就是旧的**p->sz + n**）*的虚拟地址，我们希望内核能够分配一个内存page，即在page fault handler中，通过kalloc函数分配一个内存page；初始化这个page内容为0；将这个内存page映射到user page table中；并且重新执行指令。

为什么在uvmunmap中可以直接改成continue？

Frans教授：之前的panic表明，我们尝试在释放一个并没有map的page。怎么会发生这种情况呢？唯一的原因是sbrk增加了p->sz，但是应用程序还没有使用那部分内存。因为对应的物理内存还没有分配，所以这部分新增加的内存的确没有映射关系。我们现在是lazy allocation，我们只会为需要的内存分配物理内存page。如果我们不需要这部分内存，那么就不会存在map关系，这非常的合理。相应的，我们对于这部分内存也不能释放，因为没有实际的物理内存可以释放，所以这里最好的处理方式就是continue，跳过并处理下一个page。

除上课修改的代码外，还要检查触发page fault的虚拟地址是否小于*p->sz**，*通过sbrk增加的用户进程的内存数是一个整型数而不是一个无符号整型数，可能会传入负数（即缩小用户内存）

在lazy allocation中，如果内存耗尽了该如何办？

如果内存耗尽了，一个选择是撤回page（evict page）。比如说将部分内存page中的内容写回到文件系统再撤回page。一旦你撤回并释放了page，那么你就有了一个新的空闲的page，你可以使用这个刚刚空闲出来的page，分配给刚刚的page fault handler，再重新执行指令。

撤回page的机制是least recently used（LRU），即最近最少使用。并且选择non-dirty page，它只被读过，但是没有被写过的page。（bit第7位），类似的，还有一个Access bit，在第六位，任何时候一个page被读或者被写了，这个Access bit会被设置，未access的page可以直接撤回，LRU更多的会看access bit。

如果你想知道page最近是否被使用过，你需要定时比如每100毫秒或者每秒清除Access bit，如果在下一个100毫秒这个page被访问过，那你就知道这个page在上一个100毫秒中被使用了。而Access bit为0的page在上100毫秒未被使用。这样你就可以统计每个内存page使用的频度，这是一个成熟的LRU实现的基础。（注，可以通过Access bit来决定内存page 在LRU中的排名）

### **Zero Fill On Demand**

当你查看一个用户程序的地址空间时，存在text区域，data区域，同时还有一个BSS区域（注，BSS区域包含了未被初始化或者初始化为0的全局或者静态变量）。当编译器在生成二进制文件时，编译器会填入这三个区域。text区域是程序的指令，data区域存放的是初始化了的全局变量，BSS包含了未被初始化或者初始化为0的全局变量。

因为BSS里面保存了未被初始化的全局变量，这里或许有许多许多个page，但是所有的page内容都为0。通常可以调优的地方是，我有如此多的内容全是0的page，在物理内存中，我只需要分配一个page，这个page的内容全是0。然后将所有虚拟地址空间的全0的page都map到这一个物理page上。这样至少在程序启动的时候能节省大量的物理内存分配。（这里的PTE都是只读的。）将要修改其中一个值时，在物理内存中申请一个新的内存page，将其内容设置为0，因为我们预期这个内存的内容为0。之后我们需要更新这个page的mapping关系，首先PTE要设置成可读可写，然后将其指向新的物理page。这里相当于更新了PTE，之后我们可以重新执行指令。

![img](assets\clip_image004-1729267433129-2.jpg)
 S

### **Copy On Write Fork**

当Shell处理指令时，它会通过fork创建一个子进程。fork会创建一个Shell进程的拷贝，所以这时我们有一个父进程（原来的Shell）和一个子进程。Shell的子进程执行的第一件事情就是调用exec运行一些其他程序，比如运行echo。现在的情况是，fork创建了Shell地址空间的一个完整的拷贝，而exec做的第一件事情就是丢弃这个地址空间，取而代之的是一个包含了echo的地址空间。这里看起来有点浪费。

Btw：Shell是一个用户界面，允许用户访问操作系统的服务。在操作系统中，它是介于用户和内核之间的中间层。Shell既可以提供图形用户界面（GUI），也可以提供命令行界面（CLI）。

在命令行接口中，用户通过键入命令来与系统交互。这些命令随后由Shell解释并传递给操作系统的内核去执行。

在图形用户界面（GUI）中，Shell是图形化的表现层，它可以提供窗口、按钮、菜单和其他控件，用户可以通过这些元素与操作系统交云鼎。

在物理内存中，XV6中的Shell通常会有4个page，当调用fork时，基本上就是创建了4个新的page，并将父进程page的内容拷贝到4个新的子进程的page中。

![img](assets\clip_image006-1729267433129-3.jpg)
 但是之后，一旦调用了exec，我们又会释放这些page，并分配新的page来包含echo相关的内容。所以对于这个特定场景有一个非常有效的优化：当我们创建子进程时，与其创建，分配并拷贝内容到新的物理内存，其实我们可以直接共享父进程的物理内存page。所以这里，我们可以设置子进程的PTE指向父进程对应的物理内存page。

当然，再次要提及的是，我们这里需要非常小心。因为一旦子进程想要修改这些内存的内容，相应的更新应该对父进程不可见，因为我们希望在父进程和子进程之间有强隔离性，所以这里我们需要更加小心一些。为了确保进程间的隔离性，我们可以将这里的父进程和子进程的PTE的标志位都设置成只读的。

![img](assets\clip_image008-1729267433129-4.jpg)
 在某个时间点，当我们需要更改内存的内容时，我们会得到page fault。因为父进程和子进程都会继续运行，而父进程或者子进程都可能会执行store指令来更新一些全局变量，这时就会触发page fault，因为现在在向一个只读的PTE写数据。

在得到page fault之后，我们需要拷贝相应的物理page。假设现在是子进程在执行store指令，那么我们会分配一个新的物理内存page，然后将page fault相关的物理内存page拷贝到新分配的物理内存page中，并将新分配的物理内存page映射到子进程。这时，新分配的物理内存page只对子进程的地址空间可见，所以我们可以将相应的PTE设置成可读写，并且我们可以重新执行store指令。实际上，对于触发刚刚page fault的物理page，因为现在只对父进程可见，相应的PTE对于父进程也变成可读写的了。（孩子都修改了父亲自然也可以修改）

![img](assets\clip_image010-1729267433129-5.jpg)
 所以现在，我们拷贝了一个page，将新的page映射到相应的用户地址空间，并重新执行用户指令。重新执行用户指令是指调用userret函数。

当发生page fault时，我们其实是在向一个只读的地址执行写操作。内核如何能分辨现在是一个copy-on-write fork的场景，而不是应用程序在向一个正常的只读地址写数据。是不是说默认情况下，用户程序的PTE都是可读写的，除非在copy-on-write fork的场景下才可能出现只读的PTE？

Frans教授：内核必须要能够识别这是一个copy-on-write场景。几乎所有的page table硬件都支持了这一点。我们之前并没有提到相关的内容，下图是一个常见的多级page table。对于PTE的标志位，我之前介绍过第0bit到第7bit，但是没有介绍最后两位RSW。这两位保留给supervisor software使用，supervisor softeware指的就是内核。内核可以随意使用这两个bit位。所以可以做的一件事情就是，将bit8标识为当前是一个copy-on-write page。

在copy-on-write lab中，还有个细节需要注意。目前在XV6中，除了trampoline page外，一个物理内存page只属于一个用户进程。trampoline page永远也不会释放，所以也不是什么大问题。但是对于这里的物理内存page，现在有多个用户进程或者说多个地址空间都指向了相同的物理内存page，举个例子，当父进程退出时我们需要更加的小心，因为我们要判断是否能立即释放相应的物理page。如果有子进程还在使用这些物理page，而内核又释放了这些物理page，我们将会出问题。那么现在释放内存page的依据是什么呢？

我们需要对于每一个物理内存page的引用进行计数，当我们释放虚拟page时，我们将物理内存page的引用数减1，如果引用数等于0，那么我们就能释放物理内存page。所以在copy-on-write lab中，你们需要引入一些额外的数据结构或者元数据信息来完成引用计数。

### **Demand Paging**

在未修改的XV6中，操作系统会加载程序内存的text，data区域，并且以eager的方式将这些区域加载进page table。应该再等等，直到应用程序实际需要这些指令的时候再加载内存，并不一定需要将整个二进制都加载到内存中。所以对于exec，在虚拟地址空间中，我们为text和data分配好地址段，但是相应的PTE并不对应任何物理内存page。对于这些PTE，我们只需要将valid bit位设置为0即可。

 

应用程序是从地址0开始运行。text区域从地址0开始向上增长。位于地址0的指令是会触发第一个page fault的指令，因为我们还没有真正的加载内存。该如何处理这里的page fault呢？首先我们可以发现，这些page是on-demand page。我们需要在某个地方记录了这些page对应的程序文件，我们在page fault handler中需要从程序文件中读取page数据，加载到内存中；之后将内存page映射到page table；最后再重新执行指令。在最坏的情况下，用户程序使用了text和data中的所有内容，那么我们将会在应用程序的每个page都收到一个page fault。但是如果我们幸运的话，用户程序并没有使用所有的text区域或者data区域，那么我们一方面可以节省一些物理内存，另一方面我们可以让exec运行的更快（注，因为不需要为整个程序分配内存）。

### **Memory Mapped Files**

核心思想是，将完整或者部分文件加载到内存中，这样就可以通过内存地址相关的load或者store指令来操纵文件。为了支持这个功能，一个现代的操作系统会提供一个叫做mmap的系统调用。这个系统调用会接收一个虚拟内存地址（VA），长度（len字节数），protection（比如只读或），一些标志位（私有or共享），一个打开文件的文件描述符，和偏移量（offset）。

假设文件内容是读写并且内核实现mmap的方式是eager方式（不过大部分系统都不会这么做），内核会从文件的offset位置开始，将数据拷贝到内存，设置好PTE指向物理内存的位置。之后应用程序就可以使用load或者store指令来修改内存中对应的文件内容。当完成操作之后，会有一个对应的unmap系统调用，参数是虚拟地址（VA），长度（len）。来表明应用程序已经完成了对文件的操作，在unmap时间点，我们需要将dirty block写回到文件中。我们可以很容易的找到哪些block是dirty的，因为它们在PTE中的dirty bit为1。

当然，在任何聪明的内存管理机制中，所有的这些都是以lazy的方式实现。你不会立即将文件内容拷贝到内存中，而是先记录一下这个PTE属于这个文件描述符。相应的信息通常在VMA结构体中保存，VMA全称是Virtual Memory Area。例如对于这里的文件f，会有一个VMA，在VMA中我们会记录文件描述符，偏移量等等，这些信息用来表示对应的内存虚拟地址的实际内容在哪，这样当我们得到一个位于VMA地址范围的page fault时，内核可以从磁盘中读数据，并加载到内存中。所以这里回答之前一个问题，dirty bit是很重要的，因为在unmap中，你需要向文件回写dirty block。

想象一下你有一本非常厚的参考书，你经常需要查阅书中的不同章节。每一次查找，如果你把需要的页撕下来放到你的桌上，你的桌子将很快就满了，你还需要把这些页逐一还原回书里，这个过程很麻烦也很费时间。

这就像传统的读写操作，你每次需要数据时，操作系统就把数据从硬盘拷贝到内存中的一个区域（桌上），用完后又可能需要写回到硬盘上。

现在，用`mmap`的话，就好像你用一种魔法方式把整本书映射到你房间的一面墙上。这面墙其实是个虚拟的显示屏，你想看书的哪一页，只需要对着墙上的对应位置看一眼，页面就像显现一样出现在那儿，不需要把真实的纸页搬来搬去。

`mmap`可以直接访问文件内容，就像这些内容已经在内存中一样——它就在那面“魔法墙”上随时可见。而实际上，书本身还在原来的地方，操作系统会在幕后处理所有需要让页面显示出来的工作，比如从硬盘上获取数据。当你完成阅读时，也不需要像传统读写那样清理桌面，因为你实际上并没有把数据搬来搬去。所以，`mmap`就是一种让文件内容直接在内存的地址空间中出现的技术，无需通过传统的读写进行物理的数据移动。这样既方便又高效，尤其是在频繁访问大文件的情况下。

使用`mmap`映射文件时，这个文件会被映射到调用进程的虚拟地址空间中的一个区域。随后，进程可以像访问常规内存一样访问这块区域。当进程访问这块区域的（虚拟）地址时，它实际上可能在物理内存中，也可能暂时在磁盘上（如果操作系统已经将对应的内存页交换出去或尚未加载该页）。

在对大型文件进行频繁且散布的随机访问时，`mmap`可能提供更好的性能，因为它减少了系统调用的开销，并且允许操作系统更高效地管理文件缓存。而对于小文件或只需要简单顺序访问的场景，传统的`read`/`write`方法通常更直接和简单。其中一个原因为：内存映射会占据进程的地址空间。即使小文件本身不占用太多物理内存，但因为映射必须是页（通常是4KB或更大）的倍数，这可能导致对于非常小的文件来说存在内存的浪费。

选择哪种方式取决于特定应用程序的需求和性能特点。在使用`mmap`时，我们不需要进行读取或写入调用。代替的是，我们通过直接访问映射后的内存区域来修改数据。在`read`/`write`中，我们使用了系统调用`read`和`write`分别读取和写入数据。每次调用都需要内核参与，导致用户空间和内核空间之间进行数据拷贝。



## Lab内容

我要用系统调用sbrk，那肯定不可避免走到usertrap函数里面执行东西

读入的虚拟地址比p->sz大、读入的虚拟地址比进程的用户栈小、申请空间不够、映射失败的时候都需要终止进程。所以干脆写if (va < p->sz && va > PGROUNDDOWN(p->trapframe->sp))

 

当调用系统调用(如read、write)的时候，内核会访问未被分配的页表，此时不会进入usertrap（因为usertrap是用户模式异常处理程序）， 所以这时需要分配内存。查看read write系统调用，发现最后在walkaddr函数，该函数实现的功能是：PTE无效、不存在、无PTE_U标志位时，都会返回0表示失败。现在我们需要添加Lazy allocation功能，所以PTE无效、不存在时需要分配、映射内存。

上面红色那句，需要注意此时va是在heap处，所以判断条件要修改下if(va >= PGROUNDUP(p->trapframe->sp) && va < p→sz)

和用户态的trap是不同的处理方式